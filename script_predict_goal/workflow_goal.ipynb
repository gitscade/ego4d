{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal Prediction\n",
    "### 1. Load Vectorstore\n",
    "- Manage goal predicition module separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goalstep vids: 717 and spatial vids: 36\n",
      "goalstep_document_list: 39979\n",
      "spatial_document_list: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "# Embedding Database\n",
    "import database\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('/usr/local/lib/python3.10/dist-packages'))\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# configure paths\n",
    "data_path = '../data/ego4d_annotation/'\n",
    "goalstep_annotation_path = data_path + 'goalstep/'\n",
    "spatial_annotation_path = data_path + 'spatial/'\n",
    "\n",
    "# extract videos list\n",
    "goalstep_videos_list = database.merge_json_video_list(goalstep_annotation_path)\n",
    "spatial_videos_list = database.merge_json_video_list(spatial_annotation_path)\n",
    "print(f\"goalstep vids: {len(goalstep_videos_list)} and spatial vids: {len(spatial_videos_list)}\")\n",
    "\n",
    "# make document list from the video list\n",
    "goalstep_document_list = database.make_goalstep_document_list(goalstep_videos_list)\n",
    "spatial_document = database.make_spatial_document_list(spatial_videos_list)\n",
    "print(f\"goalstep_document_list: {len(goalstep_document_list)}\")\n",
    "print(f\"spatial_document_list: {len(spatial_document)}\")\n",
    "\n",
    "# make vectorstores for each dataset (takes most time-3min)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "goalstep_vector_store =  DocArrayInMemorySearch.from_documents(goalstep_document_list, embedding=embeddings)\n",
    "spatial_vector_store = DocArrayInMemorySearch.from_documents(spatial_document, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve with Query made with Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/project/script_predict_goal\n",
      "metrics.pssSy\n",
      "[Document(metadata={'type': 'level2', 'video_uid': '543e4c99-5d9f-407d-be75-c397d633fe56', 'start_time': 0.62112, 'end_time': 15.483179999999999, 'step_category': 'Make recipes: Take two slices of bread and put them on a plate', 'step_description': 'Place two slices on bread on the plate'}, page_content='Level 2 Segment 1 for Video 543e4c99-5d9f-407d-be75-c397d633fe56\\nStep: Place two slices on bread on the plate'), Document(metadata={'type': 'level3', 'video_uid': '7921902b-293f-49e2-9401-d96791a90e15', 'parent_level1_start_time': 137.46841, 'start_time': 142.81772, 'end_time': 188.65649, 'step_category': 'Make recipes: Take two slices of bread and put them on a plate', 'step_description': 'get two pieces of  sliced bread '}, page_content='Level 3 Segment 1 for Level 2 Segment 3 in Video 7921902b-293f-49e2-9401-d96791a90e15\\nStep: get two pieces of  sliced bread '), Document(metadata={'type': 'level2', 'video_uid': '94d5eff8-0fac-4719-adf2-5c0208ab89f7', 'start_time': 810.5930050000001, 'end_time': 849.158, 'step_category': 'General cooking activity: Organize and arrange cooking tools or utensils', 'step_description': 'Arrange some items'}, page_content='Level 2 Segment 27 for Video 94d5eff8-0fac-4719-adf2-5c0208ab89f7\\nStep: Arrange some items'), Document(metadata={'type': 'level2', 'video_uid': 'bb2297d3-91b7-4ec4-b39b-cc64694929db', 'start_time': 214.58025, 'end_time': 217.11743, 'step_category': 'General kitchen activity: Set the dining table', 'step_description': 'Arrange a bottle on a table'}, page_content='Level 2 Segment 5 for Video bb2297d3-91b7-4ec4-b39b-cc64694929db\\nStep: Arrange a bottle on a table')]\n",
      "[Document(metadata={'type': 'level2', 'video_uid': '737e9619-7768-407c-8a4f-6fe1e8d61f04', 'number': 7, 'level': 2}, page_content=\"Level 2 Segment 8 for level 1 737e9619-7768-407c-8a4f-6fe1e8d61f04\\nContext: {'player': [{'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'soup', 'status': 'cooked'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'soup', 'status': 'cooked'}}], 'change': []}\"), Document(metadata={'type': 'level3', 'video_uid': '46e07357-6946-4ff0-ba36-ae11840bdc39', 'number': 1, 'level': 3}, page_content=\"Level 3 Segment 2 for Level 2 Segment 6 in Video 46e07357-6946-4ff0-ba36-ae11840bdc39\\nContext: {'player': [{'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'change', 'target': {'type': 'avatar', 'name': 'player', 'status': 'stand'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 17, 'name': 'sink', 'status': 'supply water'}}, {'entity': {'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'default'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'in water'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'in water'}}, {'entity': {'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'shake'}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'in water'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'default'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'washed'}}], 'change': [{'entity': {'type': 'item', 'id': 17, 'name': 'sink', 'status': 'default'}, 'relation': 'change', 'target': {'type': 'item', 'id': 17, 'name': 'sink', 'status': 'supply water'}}, {'entity': {'entity': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'default'}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'default'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'in water'}}, {'entity': {'entity': {'type': 'item', 'id': 16, 'name': 'sieve', 'status': 'in water'}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'in water'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 15, 'name': 'sweet corn', 'status': 'washed'}}]}\"), Document(metadata={'type': 'level2', 'video_uid': '737e9619-7768-407c-8a4f-6fe1e8d61f04', 'number': 8, 'level': 2}, page_content=\"Level 2 Segment 9 for level 1 737e9619-7768-407c-8a4f-6fe1e8d61f04\\nContext: {'player': [{'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'change', 'target': {'type': 'avatar', 'name': 'player', 'status': 'stand'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 13, 'name': 'jute leaves', 'status': 'sliced'}}, {'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'has', 'target': {'type': 'item', 'id': 13, 'name': 'jute leaves', 'status': 'sliced'}}, 'relation': 'change', 'target': {'type': 'avatar', 'name': 'player', 'status': 'stand'}}], 'change': [{'entity': {'entity': {'entity': {'type': 'item', 'id': 10, 'name': 'table', 'status': 'default'}, 'relation': 'has', 'target': {'type': 'item', 'id': 17, 'name': 'bowl', 'status': 'default'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 17, 'name': 'bowl', 'status': 'half-filled'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 13, 'name': 'jute leaves', 'status': 'sliced'}}]}\"), Document(metadata={'type': 'level2', 'video_uid': '46e07357-6946-4ff0-ba36-ae11840bdc39', 'number': 2, 'level': 2}, page_content=\"Level 2 Segment 3 for level 1 46e07357-6946-4ff0-ba36-ae11840bdc39\\nContext: {'player': [{'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'change', 'target': {'type': 'avatar', 'name': 'player', 'status': 'stand'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'contain water'}}, {'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'has', 'target': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'contain water'}}, 'relation': 'has', 'target': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'tilted'}}, {'entity': {'entity': {'type': 'avatar', 'name': 'player', 'status': 'stand'}, 'relation': 'has', 'target': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'tilted'}}, 'relation': 'change', 'target': {'type': 'avatar', 'name': 'player', 'status': 'drink'}}], 'change': [{'entity': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'contain water'}, 'relation': 'change', 'target': {'type': 'item', 'id': 9, 'name': 'a bottle of water', 'status': 'tilted'}}]}\")]\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "import sys\n",
    "import input_source\n",
    "sys.path.append(os.path.abspath('/root/project')) # add root path to sys.path for external package\n",
    "from util import metrics\n",
    "\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# can import other scripts in other folders fine!\n",
    "metrics.printtest()\n",
    "\n",
    "# Input from Source Space (context & lv2 actions)\n",
    "input_video_idx = 0\n",
    "goalstep_video = goalstep_videos_list[input_video_idx]\n",
    "spatial_video = spatial_videos_list[input_video_idx]\n",
    "input_goalstep_segments = input_source.extract_lev2_goalstep_segments(goalstep_video)\n",
    "input_spatial_context = input_source.extract_spatial_context(spatial_video)\n",
    "\n",
    "# Create retrievers for both vectorstores\n",
    "goalstep_retriever = goalstep_vector_store.as_retriever()\n",
    "spatial_retriever = spatial_vector_store.as_retriever()\n",
    "\n",
    "# do not use multiretriever since there are too much errors!\n",
    "query = f\"Here is the {input_goalstep_segments} and {input_spatial_context}\"\n",
    "retrieved_stuff1 = goalstep_retriever.get_relevant_documents(query)\n",
    "retrieved_stuff2 = spatial_retriever.get_relevant_documents(query)\n",
    "print(retrieved_stuff1)\n",
    "print(retrieved_stuff2)\n",
    "\n",
    "# Retrieve documents' parent documents for goalstep annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Prompt and Invoke LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "import prompt_source as promptSource\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# define llm\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai.api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# define prompt\n",
    "prompt = ChatPromptTemplate.from_template(promptSource.template_source)\n",
    "prompt.format(context=promptSource.context, question=promptSource.question, rules = promptSource.rules)\n",
    "\n",
    "# Define chain\n",
    "parser = StrOutputParser()\n",
    "chain1 = prompt | model | parser \n",
    "\n",
    "# Get Respone\n",
    "response = chain1.invoke()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Response and Compute Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess answers to get goals\n",
    "\n",
    "\n",
    "\n",
    "# postprocess answers to get lv1 steps?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Prompt2 for Target Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_target\n",
    "import prompt_target\n",
    "\n",
    "# target space input\n",
    "input_target_goalstep_segments = input_target.extract_lev2_goalstep_segments()\n",
    "input_target_spatial_context = input_target.extract_spatial_context()\n",
    "\n",
    "# make target query\n",
    "\n",
    "\n",
    "# retrieve with target query\n",
    "\n",
    "\n",
    "# make prompt with query and target inptu\n",
    "\n",
    "\n",
    "# make chain & invoke\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Combined chain for simple one-go-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # chain can incorpoate other chains\n",
    "# chain_action = (\n",
    "#     {\"source_spatial_context\": itemgetter(source_spatial_context), \"goalstep\": chain1, \"target_spatial_context\": itemgetter(target_spatial_context)} | prompt_action | model | parser\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "import prompt_source as promptSource\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# define llm\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=openai.api_key, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# define prompt\n",
    "prompt = ChatPromptTemplate.from_template(promptSource.template_source)\n",
    "prompt.format(context=promptSource.context, question=promptSource.question, rules = promptSource.rules)\n",
    "\n",
    "# Define chain\n",
    "parser = StrOutputParser()\n",
    "chain1 = prompt | model | parser \n",
    "\n",
    "# Get Respone\n",
    "response = chain1.invoke()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
