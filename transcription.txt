Välkommen till Kungliga vetenskap Sakademin och när presskonferensen då vi ska presentera årets Nobelpris i fysisk. Välkomna till presskonferens och Royal Swedish Academy of Sciences. Vad vi vill presentar är Nobelpris i fysisk. Vi vill kippa till vår tradition och börja med presentation i Swedish och då kan vi gå in i English. Och du är välkomna att få en fråga i en länge som ska gå upp. Jag heter Hans Ellegren och är ständig sektera här på Kungliga vetenskapsakademin. Till höger om mig sitter professor Ellen Wons, ordförande i Nobelkomitén för fysisk. Mot i vänster sitter professor Anders Irbeck, leda mot de välkomitén för fysisk och expert inom älnesområdet. My name is Hans Ellegren, under Secretary General of the Royal Swedish Academy of Sciences. And to my right is professor Ellen Wons, chair of the Nobelkomitén för fysisk. And to my left is professor Anders Irbeck, member of the Nobelkomitén för fysisk och expert inom fysisk. Åres pris handlar om masiner som läser. This year's price is about machines that learn. Kungliga vetenskapsakademin har beslutat att utdelar 20-24 års nobeltis i fysisk till John Hopfield, Princeton University USA och Jeffrey Hinton University of Toronto Canada. För grundlängande upptäckte raka uppfiningar så möjliggör machine inlärning och artificiella neroan nätverk. The Royal Swedish Academy of Sciences has today decided to award a 20-24 Nobel Prize in fysisk to John Hopfield, Princeton University USA och Jeffrey Hinton University of Toronto Canada, för fundational diskarbris och inventions att enable machine learning med artificial neroan nätverk. Professor Ellen Wons, vi nu har ett särskilt samarbete. Thank you. Learning is a fascinating ability of the human brain. We can recognize images and speech and associate them with memories and past experiences. Billions of neurons wire together give us unique cognitive abilities. Artificial neural networks are inspired by this network of neurons in our brains. This year's laureates for the Nobel Prize in physics, John Hopfield and Jeffrey Hinton used fundamental concepts from statistical physics to design artificial neural networks that function as associative memories and find patterns in large data sets. These artificial neural networks have been used to advance research across physics topics as diverse as particle physics, material science, and astrophysics. They have also become part of our daily lives, for instance in facial recognition and language translation. The laureates, discoveries, and inventions form the building blocks of machine learning that can aid humans in making faster and more reliable decisions, for instance when diagnosing medical conditions. However, while machine learning has enormous benefits, its rapid development has also raised concerns about our future. Collectively humans carry the responsibility for using this new technology in a safe and ethical way, for the greatest benefit of humankind. Thank you. Professor Ebeck, are you ready to give a more detailed presentation? Thank you. Please. So this year's Nobel Prize in physics is about our artificial neural networks. Today we know that this is a powerful computational approach. This was not evident 50 years ago, but it was known that we mammals are very good at pattern recognition by some sort of computation in our brains. And this sparked an interest in understanding the collective properties of networks of simplified neurons, connected by couplings with a strength that could become weaker or stronger. And the idea would then be to determine the strengths of the couplings to achieve a certain function, and doing that by training the network on many examples. A breakthrough came in 1982, when John Hopfield presented a dynamic network, which could store and retrieve memories, an associative memory. The memory had simple binary zero-one nodes, all nodes connected, parabys connected. The states that remained unchanged with time were identified as memories. Moreover, it was possible to introduce an energy similar to an energy one has in studying magnetic systems in physics. And that energy had the property that it was low in the state's corresponding to memories. Metaphorically, the memories were located in the list of energy landscape. When starting from this stupid pattern with a higher energy, it would slide down the network, would slide down in energy to a nearby valley. And by this process, this stupid pattern could be corrected. In follow-up work, John Hopfield also showed that this network was robust in the sense that the binary nodes could be replaced with analog ones, and he also showed how the network could be used to solve difficult optimization problems. The creation and explorations of this network by John Hopfield was milestone in our understanding of the computational abilities of artificial neural networks. Another important discovery came soon afterwards by Joffrey Hinton and Terence Seynopski. They created a stochastic version of the Hopfield network based on statistical physics and called the Boltzmann machine. So here, the focus is on statistical distributions of patterns rather than individual patterns. It is generative model. Once trained, it can be used to generate new instances from the learned distribution. It had the same basic structure as Hopfield's network, but there were two types of nodes, hidden and visible ones, and the hidden nodes were there to make it possible for the network to learn more generative distributions. While theoretically interesting, in practice, the Boltzmann machine was initially of limited use. It was prohibitively demanding computationally. However, a version with fewer couplings, called the restrict Boltzmann machine, developed into a versatile tool, and Icon will soon mention it again. So far, I talked about recurrent networks with feedback connections. Many of today's deep learning methods involved feedback networks, where information flow from an input layer to an output layer via hidden layers. The 1980s hint on how such a network with hidden layers could be trained, and in that process, he also elucidated the important function of hidden layers. Then in the 1990s, there were applications of multi-layer networks, successful applications, for example, for the classification of handwritten digits, but the networks that one called Train had relatively few couplings between consecutive layers. It remained a challenge to train more general deep structures with high connectivity between the layers. Here, in fact, many gave up, but hint on did not. And hint on, overcame this barrier by using this restricted Boltzmann machine. He used it to pre-train deep structures, and by this method, he succeeded in implementing examples of deep and dense structures. Which was a breakthrough toward deep learning. Finally, so now I have talked about physics, how physics has been a driving force in innovation and development. It's also interesting to see how physics as a research field is benefiting from these methods. One very established example here is data analysis in particle physics and astrophysics. An increasingly important application is in modeling materials. For example, to search for more efficient solar cells. Yet another example is in explicit physics-based climate modeling to enable higher resolution. Finally, I want to mention two applications, successful applications, outside the physics area, protein structure prediction, and analysis of medical images. Thank you for your attention. Thank you, Professor Irbeck. I think we might have John Hoppfield, or Jeff Hinton, with us on the phone. Good morning, Professor Hinton. Good morning. Please accept our warmest congratulations to receiving the Nobel Prize in physics. Thank you very much. How do you feel right now? I'm flabbergasted. I had no idea this would happen. I'm very surprised. I could imagine. I'm sitting here in this beautiful session hall of the Royal Swedish Academy of Sciences. Here at the press conference, there are many interested journalists from both the Swedish and the international press. Would you be ready to take some questions from them? Yes. Yes, please. Sorry, is there anyone? Thank you. My warmest congratulations to your achievements. And to this year's Nobel Prize in physics. My name is Susan Ritzin. And my question comes from Swedish television. I know many of our viewers, also lay people are very curious about the discovery of the word it here today. I wonder, do you remember when you realized your breakthrough, awarded today, if you can bring us back in time briefly? And what were the reasons for, or the inspiration for these revelations? So I remember a couple of occasions with two of my mentors. So I have an enormous debt to David Rommelhardt and Teresyn Offsky. With David Rommelhardt, we discovered the back propagation algorithm. And that within the beginning of 1982. And with Teresyn Offsky, Teri and I discovered a learning algorithm for hotfield nets that had hidden units. And I remember very well going to a meeting in Rochester where John Hopfield talked. And we, I first learned about the hotfield energy function for neural networks. And after that, Teri and I worked feverishly on how to generalize neural networks to have hidden units. And at the beginning of 1982, we came up with a learning algorithm for ultimate machines, which are hotfield nets with hidden units. So the most exciting times were with David Rommelhardt and back propagation in Teresyn Offsky on both of the machines. Thank you. Okay. More questions? First here. Hello, Bogin Radecki from the Polish Television. Congratulations. The question I have is a little bit about the future because obviously we are very excited about what neural networks and machine learning can do now. But what we're even more excited is the prospect of what they could do in the future. What are your predictions about the degree of influence that this technology is going to have on our civilization? I think it will have a huge influence. It will be comparable with the industrial revolution. But instead of exceeding people in physical strength, it's going to exceed people in intellectual ability. We have no experience of what it's like to have things smarter than us. And it's going to be wonderful in many respects. In areas like healthcare, it's going to give us much better healthcare. In almost all interest, it's going to make them more efficient. People are going to be able to do the same right to work with an AI assistant in much less time. It'll mean huge improvements in productivity. But we also have to worry about a number of possible bad consequences, particularly the threat of these things getting out of control. I think first we have here and then there. Hi, I'm Stelimon Kampanello with the audience. Congratulations on the price. My question is, last year you set in an interview with New York Times that you regret part of your life's work because of their risks with artificial intelligence. How do you feel about today? There's two kinds of regrets. There's regrets where you feel guilty because you did something you shouldn't have done. And then there's regret where you did something that you would do again in the same circumstances. But it may in the end not turn out well. That second kind of regret I have. In the same circumstances, I would do the same again. But I am worried that the overall consequence of this might be systems more intelligent than us that eventually take control. Yes, please. Hi, congratulations on the price. My name is Almyn Niedner. I'm from TV4, Swedish TV Channel. I would like to develop this old smart machine. And I wonder what's the time of AI? What type of AI do you... Has come out of it like a GPT or how you see breast cancer in X-rays or how you make funny pictures in Dali. What kind of AI do your research? Do build on your research? So there were two different learning algorithms I was involved in. One was the Boltzmann machine, which was a learning algorithm for popular networks with hidden units. And we did eventually find a practical version of that. But that's not what's led to the main progress currently in your networks. That's the back propagation algorithm. And this is a way to get a neural net to learn anything. And it's the back propagation algorithm that's led to the huge surge in AI applications and in the ability to recognize images and understand speech and to deal with natural language. It's not the Boltzmann machine that did that. It's the back propagation algorithm. More questions? One here please. Hi, my name is Bill. I'm from the Swedish paper and a technique. Do you have any favorite AI tool that you use? I actually use GPT-4 quite a lot. Whenever I want to know the answer to anything, I just go and ask GPT-4. I don't totally trust it because it can hallucinate. But on almost everything, it's a not very good expert. And that's very useful. Okay. Don't see any more. Is there one more hand there? Please. Yes, hello. Congratulations, Paul Reese from Al Jazeera English. Could you just give us a sense of where you were when you got the call? How it affected you? Is this the day you have in your diary just in case you get that call or is it a bolt from the blue? It was a bolt from the blue. It's a cheap hotel in California that doesn't have an internet connection and doesn't have a very good phone connection. I was going to get an MRI scan today, but I think I'll have to cancel that. Okay, this seems to be the last question from the press for you, Professor Hinton. Thank you. Thank you. And once again, our warmest congratulations. We look forward to seeing you here in Stockholm in December for the Nobel Prize ceremony. Thank you. Okay. So let's move on to more questions about the physics prize and the research involved. Or if you want to ask the committee members questions about their work. Questions are welcome in either English or Swedish. Please. Yes. If these two scientists wouldn't have existed, would we have like GPT then? Professor Mons, do you want to address that? That's a very good difficult question to answer because it's hard to imagine. They have contributed, of course, enormously, very early in the progress of this technology. So in the 80s, these first steps were taken. And later on, other scientists have built upon these developments. So in a sense, it may have been difficult without those groundbreaking first discoveries and inventions. I'm thinking since John Jay Hopfield is not here, I was wondering a little bit about what makes you most... What do you think is the most exciting part of his discoveries? Professor Ibeck, do you want to rest at? When it comes to his network, he was... Parts of it had been discussed earlier, but he was able to put pieces together to have created a network with a clear function and clear principles for how it worked. And it meant a lot in the field. One more question here? Oh, sorry, yes. I also wonder about the worries that Jeffrey Hinton expressed. What are your worries about this technology? Professor Mons, do you want to rest? Well, these type of worries are expressed a lot and discussed in the scientific community. And I think it is very good that they are discussed and it contributes to the knowledge about machine learning in the society. I think it is important that as many people as possible learn about the mechanisms of machine learning so that it is not just in the hands of a few individuals. So I think, of course, Professor Hinton is one of many who express their views on this. I think that is very good. Maybe I can add that there are, of course, many discoveries and inventions over the time that has been potentially possible to misuse, but it is a common responsibility for a society to have regulations, to avoid that to happen. And I think that would apply to artificial intelligence. I think that is it, actually, time is running out. Thank you for your interest in participating in this press conference. We hope to see you again here tomorrow when we will present the Nobel Prize in Chemistry. So thank you. Thank you. Thank you. How can I make it to the other side? I think it is very important. Yes. I was right. I was right. I was right. I don't know. I don't know. I saw him just now. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I always would like premier. Hello tea. I always wanted to see her do goodม. This together has been a dem Good moment. Hello tea. Yes girls, everyone will drink this beer. Hello tea, everyone will drink this beer. – Jag driver Bavlöda S judiciaryÅ innebär bil för att inkomligen manuskruppen som har en program med dig Frcild av idées och methods från Finland. Var ska vi ta tusen en kvinna vad det nesse som blir tilllor i vår respekt? Ja, i några mytheіл i odpowied controversy Finland, när jag ska flaggrava flagangen varayım om för att den värde brukar vi tala med ouran Oops Varing inte, när den kommer och får Nural Networks. I kredit. Modern. With element som vitt var similar to what one has in magnetic models in physics, but this was a new thing for neural networks. And it was It was good because it put together different elements. It was, he gave the network a clear function and it worked after, according to clear principles. All right. And created what part was the sort of the memory part? Yes, yes, this was the associative memory. Exactly. Yes. And the other one already? Ej, jefer Hinton. Jämt. Very soon afterwards he created a model directly based on, it based on the hope for network, but he changed so that the focus was now not on individual memories individual patterns, but on statistical distributions of patterns. That was one thing in the 80s by Jeff Hinton. He also created a learning algorithm for what is called Fried multilayered networks, Friedforward multilayered networks. What could that do? Ej, so the whole idea in artificial neural network is that I have my system of nodes or neurons. They are connected by couplings of and those can have different strengths. And one that in order to achieve some function, one has to train these the network on many examples and training means that one tries to determine good values for these couplings. And this is complex because there are a lot of couplings in such a network. And he, Hinton created learning algorithm for this whole smart machine and for multinein layer, feed forward structures. And that worked through very important contributions. And these are some important contributions for what we today call AI artificial intelligence. Will you tell me something about how this I think we've all heard about it. But what would you say are the most important ways this affect us today? In many, many ways. So in not least in science, in physics and other scientific fields, it's a physics, these tools based on artificial network. They have been around for quite some time already before. It became deep learning. The Bermannian many use for tools. And now as the artificial neural network tools get more and more powerful, we steadily see new applications and material science, modeling in materials science is one important example. And outside physics, it was pointed out by Hinton himself, Indian, but certainly healthcare is very important. It is already a very good tool for analysis of medical images of different countries. Now we just heard Jeffrey Hinton here in the press conference saying he was flabbergasted. Is there anything personal you know about the laureates that you would like to share with us? That's very important to you, the same question. No, maybe not actually. I mean, personally, I wouldn't say no. No. They have both been really, I think, through pioneers. And finding new ways to tackle problems. How would you tell us in a sentence or two just when you are excited about awarding the price to this particular field this year? I think it is fantastic to create a completely new way of computing and see how it develops into such a powerful tool. Thank you. Thank you very much, Professor Amdistir Rik, member of the Nobel Physics Committee. Yep, thank you. You