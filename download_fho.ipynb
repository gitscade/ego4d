{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STA_TRAIN_PATH = \"data_recommended/v2/annotations/fho_sta_train.json\"\n",
    "STA_VAL_PATH = \"data_recommended/v2/annotations/fho_sta_val.json\"\n",
    "STA_TEST_PATH = \"data_recommended/v2/annotations/fho_sta_test_unannotated.json\"\n",
    "OBJ_DET_PATH = \"data_recommended/v2/sta_models/object_detections.json\"\n",
    "\n",
    "metadata_file_path = \"data_recommended/ego4d.json\"\n",
    "ego4d_meta = json.load(open(metadata_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9821"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_to_dims = {\n",
    "    v[\"video_uid\"]: {\n",
    "        \"frame_height\": v[\"video_metadata\"][\"display_resolution_height\"],\n",
    "        \"frame_width\": v[\"video_metadata\"][\"display_resolution_width\"],\n",
    "    }\n",
    "    for v in ego4d_meta[\"videos\"]\n",
    "}\n",
    "len(video_to_dims)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sta_train = json.load(open(STA_TRAIN_PATH))\n",
    "train_video_uids = set(x.get(\"video_id\", x.get(\"video_uid\", None)) for x in sta_train[\"annotations\"])\n",
    "\n",
    "sta_val = json.load(open(STA_VAL_PATH))\n",
    "val_video_uids = set(x.get(\"video_id\", x.get(\"video_uid\", None)) for x in sta_val[\"annotations\"])\n",
    "\n",
    "sta_test = json.load(open(STA_TEST_PATH))\n",
    "test_video_uids = set(x.get(\"video_id\", x.get(\"video_uid\", None)) for x in sta_test[\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def save_ego4d_features_to_hdf5(video_uids: List[str], feature_dir: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Use this function to preprocess Ego4D features into a HDF5 file with h5py\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    with h5py.File(out_path, \"w\") as out_f:\n",
    "        for uid in tqdm(video_uids, desc=\"video_uid\", leave=True):\n",
    "            feature_path = os.path.join(feature_dir, f\"{uid}.pt\")\n",
    "            if not os.path.exists(feature_path):\n",
    "                errors.append(uid)\n",
    "                continue\n",
    "            fv = torch.load(feature_path)\n",
    "            out_f.create_dataset(uid, data=fv.numpy())\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video_uid: 100%|██████████| 989/989 [00:00<00:00, 169638.35it/s]\n",
      "video_uid: 100%|██████████| 522/522 [00:00<00:00, 208596.29it/s]\n",
      "video_uid: 100%|██████████| 302/302 [00:00<00:00, 129042.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train_feature_path = \"data_recommended/v2/annotations/fho/features_train.hdf5\"\n",
    "train_err_video_uids = save_ego4d_features_to_hdf5(train_video_uids, feature_dir=\"data_recommended/v2/annotations/fho/omnivore_video_swinl_fp16\", out_path=train_feature_path)\n",
    "\n",
    "val_feature_path = \"data_recommended/v2/annotations/fho/features_val.hdf5\"\n",
    "val_err_video_uids = save_ego4d_features_to_hdf5(val_video_uids, feature_dir=\"data_recommended/v2/annotations/fho/omnivore_video_swinl_fp16\", out_path=val_feature_path)\n",
    "\n",
    "test_feature_path = \"data_recommended/v2/annotations/fho/features_test.hdf5\"\n",
    "test_err_video_uids = save_ego4d_features_to_hdf5(test_video_uids, feature_dir=\"data_recommended/v2/annotations/fho/omnivore_video_swinl_fp16\", out_path=test_feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 989)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_video_uids_to_use = list(set(train_video_uids) - set(train_err_video_uids))\n",
    "val_video_uids_to_use = list(set(val_video_uids) - set(val_err_video_uids))\n",
    "test_video_uids_to_use = list(set(test_video_uids) - set(test_err_video_uids))\n",
    "\n",
    "len(train_video_uids_to_use), len(set(train_err_video_uids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "def _one_hot_encoding(n, clazzes):\n",
    "    result = torch.zeros(n)\n",
    "    result[clazzes] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "class STAFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset loads STA data via loading the corresponding:\n",
    "    - Feature vector for the clip\n",
    "    - Pre-detected bounding boxes\n",
    "    - Ground truth bounding boxes, verbs, nouns and time to contact (ttc)\n",
    "    \"\"\"\n",
    "    def __init__(self, video_uids, data, feature_path, obj_det_path, det_score_threshold, next_active_threshold, keep_max_iou):\n",
    "        self.anns = [x for x in data.get(\"annotations\", data.get(\"clips\", None)) if x.get(\"video_id\", x.get(\"video_uid\", None)) in video_uids]\n",
    "        self.features = h5py.File(feature_path)\n",
    "        self.num_nouns = len(data[\"noun_categories\"])\n",
    "        self.num_verbs = len(sta_train[\"verb_categories\"])\n",
    "        self.obj_dets = json.load(open(obj_det_path))\n",
    "        self.det_score_threshold = det_score_threshold\n",
    "        self.next_active_threshold = next_active_threshold\n",
    "        self.keep_max_iou = keep_max_iou\n",
    "        # self.noun_categories = set(data[\"noun_categories\"])\n",
    "        # print(self.noun_categories)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "    \n",
    "    def _get_obj_dets(self, uid, w, h):\n",
    "        object_detections = self.obj_dets[uid]\n",
    "\n",
    "        # taken from:\n",
    "        # https://github.com/EGO4D/forecasting/blob/main/ego4d/datasets/short_term_anticipation.py#L699\n",
    "        if len(object_detections) > 0:\n",
    "            pred_boxes = np.vstack([[\n",
    "                x['box'][0] / w,\n",
    "                x['box'][1] / h,\n",
    "                x['box'][2] / w,\n",
    "                x['box'][3] / h\n",
    "              ] for x in object_detections]\n",
    "            )\n",
    "            pred_scores = np.array([x['score'] for x in object_detections])\n",
    "            pred_object_labels = np.array([x['noun_category_id'] for x in object_detections])\n",
    "\n",
    "            # exclude detections below the theshold\n",
    "            detected = (\n",
    "                pred_scores\n",
    "                >= self.det_score_threshold\n",
    "            )\n",
    "\n",
    "            pred_boxes = pred_boxes[detected]\n",
    "            pred_object_labels = pred_object_labels[detected]\n",
    "            pred_scores = pred_scores[detected]\n",
    "        else:\n",
    "            pred_boxes = np.zeros((0, 4))\n",
    "            pred_scores = pred_object_labels = np.array([])\n",
    "\n",
    "        return {\n",
    "            \"pred_boxes\": torch.tensor(pred_boxes).to(torch.float32), \n",
    "            \"pred_object_labels\": torch.tensor(pred_object_labels).to(torch.float32), \n",
    "            \"pred_scores\": torch.tensor(pred_scores).to(torch.float32),\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.anns[idx]\n",
    "        uid = ann[\"uid\"]\n",
    "        v_uid = ann.get(\"video_id\", ann.get(\"video_uid\", None))\n",
    "        start_idx = ann[\"frame\"] // 16   # TODO use start time\n",
    "        fs = torch.tensor(self.features[v_uid][start_idx]).to(torch.float32)\n",
    "        dims = video_to_dims[v_uid]\n",
    "        w, h = dims[\"frame_width\"], dims[\"frame_height\"]\n",
    "\n",
    "        boxes = []\n",
    "        verbs = []\n",
    "        nouns = []\n",
    "        contact_time = []\n",
    "        num_objs = len(ann.get(\"objects\", []))\n",
    "        for obj in ann.get(\"objects\", []):\n",
    "            boxes.append(torch.tensor([\n",
    "                float(obj[\"box\"][0] / w),\n",
    "                float(obj[\"box\"][1] / h), \n",
    "                float(obj[\"box\"][2] / w),\n",
    "                float(obj[\"box\"][3] / h),\n",
    "            ]))\n",
    "            verbs.append(_one_hot_encoding(self.num_verbs, obj[\"verb_category_id\"]))\n",
    "            nouns.append(_one_hot_encoding(self.num_nouns, obj[\"noun_category_id\"]))\n",
    "            contact_time.append(torch.tensor(float(obj[\"time_to_contact\"])))\n",
    "        \n",
    "        label_dict = {\n",
    "            \"boxes\": torch.stack(boxes) if len(boxes) > 0 else torch.empty(1),\n",
    "            \"verbs\": torch.stack(verbs) if len(verbs) > 0 else torch.empty(1),\n",
    "            \"nouns\": torch.stack(nouns) if len(nouns) > 0 else torch.empty(1),\n",
    "            \"ttc\": torch.stack(contact_time) if len(contact_time) > 0 else torch.empty(1),\n",
    "            \"num_objs\": torch.tensor(num_objs),\n",
    "            \"uids\": ann[\"uid\"],\n",
    "            \"video_uids\": ann.get(\"video_id\", ann.get(\"video_uid\", None)),\n",
    "        }\n",
    "\n",
    "        pred = self._get_obj_dets(uid, w, h)\n",
    "\n",
    "        # labelled\n",
    "        if len(boxes) > 0:\n",
    "            ious = compute_iou(pred[\"pred_boxes\"], label_dict[\"boxes\"])\n",
    "            matches = ious.argmax(-1)\n",
    "            ious = ious.max(-1)\n",
    "            if self.keep_max_iou:\n",
    "                next_active = ious >= min(ious.max() - 1e-2, self.next_active_threshold)\n",
    "            else:\n",
    "                next_active = ious >= self.next_active_threshold\n",
    "\n",
    "            # filter out next active instead of setting to nan\n",
    "            pred[\"pred_verbs\"] = label_dict[\"verbs\"][matches]\n",
    "            pred[\"pred_nouns\"] = label_dict[\"nouns\"][matches]\n",
    "            pred[\"pred_ttc\"] = label_dict[\"ttc\"][matches]\n",
    "            for k in \"pred_verbs\", \"pred_ttc\", \"pred_boxes\", \"pred_scores\", \"pred_object_labels\", \"pred_nouns\":\n",
    "                pred[k] = pred[k][next_active]\n",
    "\n",
    "        label_dict.update(pred)\n",
    "        return fs, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StaFeaturesModel(nn.Module):\n",
    "    def __init__(self, in_feature_dim, proj_feature_dim, num_nouns, num_verbs, leaky=0.2):\n",
    "        super().__init__()\n",
    "        # TODO: self attention / non local block\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_feature_dim, proj_feature_dim),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Linear(proj_feature_dim, proj_feature_dim),\n",
    "            # nn.LeakyReLU(leaky),\n",
    "        )\n",
    "        self.roi_head = nn.Sequential(\n",
    "            nn.Linear(proj_feature_dim + 4, proj_feature_dim),\n",
    "            # nn.LeakyReLU(leaky),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.verb_head = nn.Linear(proj_feature_dim, num_verbs)\n",
    "        self.noun_head = nn.Linear(proj_feature_dim, num_nouns)\n",
    "        self.ttc_head = nn.Linear(proj_feature_dim, 1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x, boxes):\n",
    "        bs = x.shape[0]\n",
    "        if boxes.shape[0] == 0:\n",
    "            return {\n",
    "                \"nouns\": torch.zeros((0, self.noun_head.out_features)),\n",
    "                \"verbs\": torch.zeros((0, self.verb_head.out_features)),\n",
    "                \"ttc\": torch.zeros((0, self.ttc_head.out_features)),\n",
    "            }\n",
    "            \n",
    "        assert len(boxes.shape) == 2 and boxes.shape[1] == 5\n",
    "        box_idx = boxes[:, 0].long()\n",
    "        box_vals = boxes[:, 1:]\n",
    "\n",
    "        p = self.proj(x)\n",
    "        p = p[box_idx]  # dupe rows for examples with multiple boxes\n",
    "        pb = torch.cat((p, box_vals), dim=-1)\n",
    "\n",
    "        r = self.roi_head(pb)\n",
    "        n = self.noun_head(r)\n",
    "        v = self.verb_head(r)\n",
    "        ttc = self.ttc_head(r)\n",
    "\n",
    "        if not self.training:\n",
    "            n = F.softmax(n, dim=-1)\n",
    "            v = F.softmax(v, dim=-1)\n",
    "        return {\"nouns\": n, \"verbs\": v, \"ttc\": ttc.squeeze()}\n",
    "    \n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(\n",
    "                module.weight.data, gain=torch.nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "def compute_iou(preds,gts):\n",
    "    \"\"\"\n",
    "    Compute a matrix of intersection over union values for two lists of bounding boxes using broadcasting\n",
    "    :param preds: matrix of predicted bounding boxes [NP x 4]\n",
    "    :param gts: number of ground truth bounding boxes [NG x 4]\n",
    "    :return: an [NP x NG] matrix of IOU values\n",
    "    \"\"\"\n",
    "    # Convert shapes to use broadcasting\n",
    "    # preds: NP x 4 -> NP x 1 x 4\n",
    "    # gts: NG x 4 -> 1 x NG x 4\n",
    "    preds = np.expand_dims(preds,1)\n",
    "    gts = np.expand_dims(gts,0)\n",
    "\n",
    "    def area(boxes):\n",
    "        width = boxes[..., 2] - boxes[..., 0] + 1\n",
    "        height = boxes[..., 3] - boxes[..., 1] + 1\n",
    "        width[width<0]=0\n",
    "        height[height<0]=0\n",
    "        return width * height\n",
    "\n",
    "    ixmin = np.maximum(gts[..., 0], preds[..., 0])\n",
    "    iymin = np.maximum(gts[..., 1], preds[..., 1])\n",
    "    ixmax = np.minimum(gts[..., 2], preds[..., 2])\n",
    "    iymax = np.minimum(gts[..., 3], preds[..., 3])\n",
    "\n",
    "    areas_preds = area(preds)\n",
    "    areas_gts = area(gts)\n",
    "    areas_intersections = area(np.stack([ixmin, iymin, ixmax, iymax], -1))\n",
    "\n",
    "    return (areas_intersections) / (areas_preds + areas_gts - areas_intersections+1e-11)\n",
    "\n",
    "\n",
    "class AbstractMeanAveragePrecision(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for implementing mAP measures\n",
    "    \"\"\"\n",
    "    def __init__(self, num_aps, percentages=True, count_all_classes=True, top_k=None):\n",
    "        \"\"\"\n",
    "        Contruct the Mean Average Precision metric\n",
    "        :param num_aps: number of average precision metrics to compute. E.g., we can compute different APs for different\n",
    "                        IOU overlap thresholds\n",
    "        :param percentages: whether the metric should return percentages (i.e., 0-100 range rather than 0-1)\n",
    "        :param count_all_classes: whether to count all classes when computing mAP. If false, classes which do not have\n",
    "                                    any ground truth label but do have associated predictions are counted (they will have\n",
    "                                    an AP equal to zero), otherwise, only classes for which there is at least one ground truth\n",
    "                                    label will count. It is useful to set this to True for imbalanced datasets for which not\n",
    "                                    all classes are in the ground truth labels.\n",
    "        :param top_k: the K to be considered in the top-k criterion. If None, a standard mAP will be computed\n",
    "        \"\"\"\n",
    "        self.true_positives = []\n",
    "        self.confidence_scores = []\n",
    "        self.predicted_classes = []\n",
    "        self.gt_classes = []\n",
    "\n",
    "        self.num_aps = num_aps\n",
    "        self.percentages = percentages\n",
    "        self.count_all_classes = count_all_classes\n",
    "        self.K = top_k\n",
    "        self.names = []\n",
    "        self.short_names = []\n",
    "\n",
    "    def get_names(self):\n",
    "        return self.names\n",
    "\n",
    "    def get_short_names(self):\n",
    "        return self.short_names\n",
    "\n",
    "    def add(self,\n",
    "            preds,\n",
    "            labels\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Add predictions and labels of a single image and matches predictions to ground truth boxes\n",
    "        :param predictions: dictionary of predictions following the format below. While \"boxes\" and \"scores\" are\n",
    "                            mandatory, other properties can be added (they can be used to compute matchings).\n",
    "                            It can also be a list of dictionaries if predictions of more than one images are being added.\n",
    "                {\n",
    "                    'boxes' : [\n",
    "                        [245,128,589,683],\n",
    "                        [425,68,592,128]\n",
    "                    ],\n",
    "                    'scores' : [\n",
    "                        0.8,\n",
    "                        0.4\n",
    "                    ],\n",
    "                    'nouns' : [\n",
    "                        3,\n",
    "                        5\n",
    "                    ],\n",
    "                    'verbs': [\n",
    "                        8,\n",
    "                        11\n",
    "                    ],\n",
    "                    'ttcs': [\n",
    "                        1.25,\n",
    "                        1.8\n",
    "                    ]\n",
    "                }\n",
    "        :param labels: dictionary of labels following a similar format. It can be a list of dictionaries.\n",
    "                {\n",
    "                    'boxes' : [\n",
    "                        [195,322,625,800],\n",
    "                        [150,300,425,689]\n",
    "                    ],\n",
    "                    'nouns' : [\n",
    "                        9,\n",
    "                        5\n",
    "                    ],\n",
    "                    'verbs': [\n",
    "                        3,\n",
    "                        11\n",
    "                    ],\n",
    "                    'ttcs': [\n",
    "                        0.25,\n",
    "                        1.25\n",
    "                    ]\n",
    "                }\n",
    "        :return matched: a list of pairs of predicted/matched gt boxes\n",
    "        \"\"\"\n",
    "        matched = []\n",
    "\n",
    "        if len(preds) > 0:\n",
    "            predicted_boxes = preds['boxes']\n",
    "            predicted_scores = preds['scores']\n",
    "            predicted_classes = self._map_classes(preds)\n",
    "\n",
    "            # Keep track of correctly matched boxes for the different AP metrics\n",
    "            true_positives = np.zeros((len(predicted_boxes), self.num_aps))\n",
    "\n",
    "            if len(labels) > 0:\n",
    "                # get GT boxes\n",
    "                gt_boxes = labels['boxes']\n",
    "\n",
    "                # IOU between all predictions and gt boxes\n",
    "                ious = compute_iou(predicted_boxes, gt_boxes)\n",
    "\n",
    "                # keep track of GT boxes which have already been matched\n",
    "                gt_matched = np.zeros((len(gt_boxes), self.num_aps))\n",
    "\n",
    "                # from highest to lowest score\n",
    "                for i in predicted_scores.argsort()[::-1]:\n",
    "                    # get overlaps related to this prediction\n",
    "                    overlaps = ious[i].reshape(-1, 1)  # NGT x 1\n",
    "\n",
    "                    # check if this prediction can be matched to the GT labels\n",
    "                    # this will give different set of matchings for the different AP metrics\n",
    "                    matchings = self._match({k: p[i] for k, p in preds.items()}, labels, overlaps)  # NGT x NR\n",
    "\n",
    "                    # replicate overlaps to match shape of matching (different AP metrics)\n",
    "                    overlaps = np.tile(overlaps, [1, matchings.shape[1]])  # NGT x NR\n",
    "\n",
    "                    # do not allow to match a matched GT boxes\n",
    "                    try:\n",
    "                        matchings[gt_matched == 1] = 0  # not a valid match #NGT x NR\n",
    "                    except:\n",
    "                        import traceback; traceback.print_exc()\n",
    "                        breakpoint()\n",
    "\n",
    "                    # remove overlaps corresponding to boxes which are not a match\n",
    "                    overlaps[matchings == 0] = -1\n",
    "\n",
    "                    jj = overlaps.argmax(0)  # get indexes of maxima wrt GT\n",
    "\n",
    "                    # get values of matching obtained at maxima\n",
    "                    # these indicate if the matchings are correct\n",
    "                    i_matchings = matchings[jj, range(len(jj))]\n",
    "\n",
    "                    jj_matched = jj.copy()\n",
    "                    jj_matched[~i_matchings] = -1\n",
    "\n",
    "                    # set true positive to 1 if we obtained a matching\n",
    "                    true_positives[i, i_matchings] = 1\n",
    "\n",
    "                    # set the ground truth as matched if we obtained a matching\n",
    "                    gt_matched[jj, range(len(jj))] += i_matchings\n",
    "\n",
    "                    matched.append(jj_matched)\n",
    "\n",
    "                # remove the K highest score false positives\n",
    "                if self.K is not None and self.K>1:\n",
    "                    # number of FP to remove:\n",
    "                    K = (self.K - 1) * len(labels['boxes'])\n",
    "                    # indexes to sort the predictions\n",
    "                    order = predicted_scores.argsort()[::-1]\n",
    "                    # sort the true positives labels\n",
    "                    sorted_tp = (true_positives[order, :]).astype(float)\n",
    "                    # invert to obtain the sorted false positive labels\n",
    "                    sorted_fp = 1 - sorted_tp\n",
    "                    # flag the first K false positives\n",
    "                    sorted_tp[(sorted_fp.cumsum(0) <= K) & (sorted_fp == 1)] = np.nan\n",
    "\n",
    "                    true_positives = sorted_tp\n",
    "                    predicted_scores = predicted_scores[order]\n",
    "                    predicted_classes = predicted_classes[order]\n",
    "\n",
    "                self.gt_classes.append(self._map_classes(labels))\n",
    "\n",
    "            # append list of true positives and confidence scores\n",
    "            self.true_positives.append(true_positives)\n",
    "            self.confidence_scores.append(predicted_scores)\n",
    "            self.predicted_classes.append(predicted_classes)\n",
    "        else:\n",
    "            if len(preds) > 0:\n",
    "                self.gt_classes.append(self._map_classes(labels))\n",
    "        if len(matched) > 0:\n",
    "            return np.stack(matched, 0)\n",
    "        else:\n",
    "            return np.zeros((0, self.num_aps))\n",
    "\n",
    "    def _map_classes(self, preds):\n",
    "        \"\"\"\n",
    "        Return the classes related to the predictions. These are used to specify how to compute mAP.\n",
    "        :param preds: the predictions\n",
    "        :return: num_ap x len(pred) array specifying the class of each prediction according to the different AP measures\n",
    "        \"\"\"\n",
    "        return np.vstack([preds['nouns']] * self.num_aps).T\n",
    "\n",
    "    def _compute_prec_rec(self, true_positives, confidence_scores, num_gt):\n",
    "        \"\"\"\n",
    "        Compute precision and recall curve from a true positive list and the related scores\n",
    "        :param true_positives: set of true positives\n",
    "        :param confidence_scores:  scores associated to the true positives\n",
    "        :param num_gt: number of ground truth labels for current class\n",
    "        :return: prec, rec: lists of precisions and recalls\n",
    "        \"\"\"\n",
    "        # sort true positives by confidence score\n",
    "        tps = true_positives[confidence_scores.argsort()[::-1]]\n",
    "\n",
    "        tp = tps.cumsum()\n",
    "        fp = (1 - tps).cumsum()\n",
    "\n",
    "        # safe division which turns x/0 to zero\n",
    "        prec = self._safe_division(tp, tp + fp)\n",
    "        rec = self._safe_division(tp, num_gt)\n",
    "\n",
    "        return prec, rec\n",
    "\n",
    "    def _safe_division(self, a, b):\n",
    "        \"\"\"\n",
    "        Divide a by b avoiding a DivideByZero exception\n",
    "        Inputs:\n",
    "            a, b: either vectors or scalars\n",
    "        Outputs:\n",
    "            either a vector or a scalar\n",
    "        \"\"\"\n",
    "        a_array = isinstance(a, np.ndarray)\n",
    "        b_array = isinstance(b, np.ndarray)\n",
    "\n",
    "        if (not a_array) and (not b_array):\n",
    "            # both scalars\n",
    "            # anything divided by zero should be zero\n",
    "            if b == 0:\n",
    "                return 0\n",
    "\n",
    "        # numerator scalar, denominator vector\n",
    "        if b_array and not a_array:\n",
    "            # turn a into a vector\n",
    "            a = np.array([a] * len(b))\n",
    "\n",
    "        # numerator vector, denominator scalar\n",
    "        if not b_array and a_array:\n",
    "            # turn a into a vector\n",
    "            b = np.array([b] * len(a))\n",
    "\n",
    "        # turn all cases in which b=0 in a 0/1 division (result is 0)\n",
    "        zeroden = b == 0\n",
    "        b[zeroden] = 1\n",
    "        a[zeroden] = 0\n",
    "        return a / b\n",
    "\n",
    "    def _compute_ap(self, prec, rec):\n",
    "        \"\"\"\n",
    "        Python implementation of Matlab VOC AP code.\n",
    "            1) Make precision monotonically decreasing 2) tThen compute AP by numerical integration.\n",
    "        :param prec: vector of precision values\n",
    "        :param rec: vector of recall values\n",
    "        :return: average precision\n",
    "        \"\"\"\n",
    "        # pad precision and recall\n",
    "        mrec = np.concatenate(([0], rec, [1]))\n",
    "        mpre = np.concatenate(([0], prec, [0]))\n",
    "\n",
    "        # make precision monotonically decresing\n",
    "        for i in range(len(mpre) - 2, 0, -1):\n",
    "            mpre[i] = np.max((mpre[i], mpre[i + 1]))\n",
    "\n",
    "        # consider only indexes in which the recall changes\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0] + 1\n",
    "\n",
    "        # compute the area uner the curve\n",
    "        return np.sum((mrec[i] - mrec[i - 1]) * mpre[i])\n",
    "\n",
    "    def _compute_mr(self, prec, rec):\n",
    "        \"\"\"\n",
    "        Compute maximum recall\n",
    "        \"\"\"\n",
    "        return np.max(rec)\n",
    "\n",
    "    def evaluate(self, measure='AP'):\n",
    "        \"\"\"\n",
    "        Compute AP/MR for all classes, then averages\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = []\n",
    "        # compute the different AP values for the different metrics\n",
    "\n",
    "        gt_classes = np.concatenate(self.gt_classes)\n",
    "        predicted_classes = np.concatenate(self.predicted_classes)\n",
    "        true_positives = np.concatenate(self.true_positives)\n",
    "        confidence_scores = np.concatenate(self.confidence_scores)\n",
    "\n",
    "        for i in range(self.num_aps):\n",
    "            # the different per-class AP values\n",
    "            measures = []\n",
    "\n",
    "            _gt_classes = gt_classes[:, i]\n",
    "            _predicted_classes = predicted_classes[:, i]\n",
    "            _true_positives = true_positives[:, i]\n",
    "            _confidence_scores = confidence_scores\n",
    "\n",
    "            if self.count_all_classes:\n",
    "                classes = np.unique(np.concatenate([_gt_classes, _predicted_classes]))\n",
    "            else:\n",
    "                classes = np.unique(_gt_classes)\n",
    "\n",
    "            # iterate over classes\n",
    "            for c in classes:\n",
    "                # get true positives and number of GT values\n",
    "                tp = _true_positives[_predicted_classes == c]\n",
    "                cs = _confidence_scores[_predicted_classes == c]\n",
    "                ngt = np.sum(_gt_classes == c)\n",
    "\n",
    "                # check if the list of TP is non empty\n",
    "                if len(tp) > 0:\n",
    "                    # remove invalid TP values and related confidence scores\n",
    "                    valid = ~np.isnan(tp)\n",
    "                    tp, cs = tp[valid], cs[valid]\n",
    "                # if both TP and GT are non empty, then compute AP\n",
    "                if len(tp) > 0 and ngt > 0:\n",
    "                    prec, rec = self._compute_prec_rec(tp, cs, ngt)\n",
    "                    if measure=='AP':\n",
    "                        this_measure = self._compute_ap(prec, rec)\n",
    "                    elif measure=='MR': #maximum recall\n",
    "                        this_measure = self._compute_mr(prec, rec)\n",
    "                    # turn into percentage\n",
    "                    if self.percentages:\n",
    "                        this_measure = this_measure * 100\n",
    "                    # append to the list\n",
    "                    measures.append(this_measure)\n",
    "                # if both are empty, the AP is zero\n",
    "                elif not (len(tp) == 0 and ngt == 0):\n",
    "                    measures.append(0)\n",
    "            # append the mAP value\n",
    "            metrics.append(np.mean(measures))\n",
    "\n",
    "        # return single value or list of values\n",
    "        values = list(metrics)\n",
    "        if len(values) == 1:\n",
    "            return values[0]\n",
    "        else:\n",
    "            return tuple(values)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _match(self, pred, gt_predictions, ious):\n",
    "        \"\"\"\n",
    "        Return matches of a given prediction to a set of GT labels\n",
    "        :param pred: the prediction dictionary\n",
    "        :param gt_predictions: the gt predictions dictionary\n",
    "        :param ious: the computed IOU matrix (NGT x NPRED)\n",
    "        :return: a num_preds x num_ap matrix specifying possible matchings depending on the prediction and metric\n",
    "        \"\"\"\n",
    "\n",
    "class ObjectOnlyMeanAveragePrecision(AbstractMeanAveragePrecision):\n",
    "    def __init__(self, iou_threshold=0.5, top_k=3, count_all_classes=False):\n",
    "        \"\"\"\n",
    "        Construct the object only mAP metric. This will compute the following metrics:\n",
    "            - Box + Noun\n",
    "            - Box\n",
    "        :param iou_threshold:\n",
    "        :param tti_threshold:\n",
    "        :param top_k:\n",
    "        :param count_all_classes:\n",
    "        \"\"\"\n",
    "        super().__init__(2, top_k=top_k, count_all_classes=count_all_classes)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.names = [\"Box + Noun mAP\", \"Box AP\"]\n",
    "        self.short_names = [\"map_box_noun\", \"ap_box\"]\n",
    "\n",
    "    def _map_classes(self, preds):\n",
    "        \"\"\"\n",
    "        Associates each prediction to a class\n",
    "        :param preds: the input predictions\n",
    "        :return the matrix of classess associated to each prediction according to the evaluation measure\n",
    "        \"\"\"\n",
    "        nouns = preds['nouns']\n",
    "        boxes = np.ones(len(preds['nouns']))\n",
    "\n",
    "        return np.vstack([\n",
    "            nouns,  # box + noun, average over nouns\n",
    "            boxes]  # box, just compute a single AP\n",
    "        ).T\n",
    "\n",
    "    def _match(self, pred, gt_predictions, ious):\n",
    "        \"\"\"\n",
    "        Return matches of a given prediction to a set of GT predictions\n",
    "        :param pred: the prediction dictionary\n",
    "        :param gt_predictions: the gt predictions dictionary\n",
    "        :param ious: the computed IOU matrix (NGT x NPRED)\n",
    "        :return: a num_preds x num_ap matrix specifying possible matchings depending on the prediction and metric\n",
    "        \"\"\"\n",
    "        nouns = (pred['nouns'] == gt_predictions['nouns'])\n",
    "        boxes = (ious.ravel() > self.iou_threshold)\n",
    "\n",
    "        map_box_noun = boxes & nouns\n",
    "        map_box = boxes\n",
    "        # breakpoint()\n",
    "\n",
    "        return np.vstack([map_box_noun, map_box]).T\n",
    "\n",
    "class OverallMeanAveragePrecision(AbstractMeanAveragePrecision):\n",
    "    \"\"\"Compute the different STA metrics based on mAP\"\"\"\n",
    "    def __init__(self, iou_threshold=0.5, ttc_threshold=0.25, top_k=5, count_all_classes=False):\n",
    "        \"\"\"\n",
    "        Construct the overall mAP metric. This will compute the following metrics:\n",
    "            - Box AP\n",
    "            - Box + Noun AP\n",
    "            - Box + Verb AP\n",
    "            - Box + TTC AP\n",
    "            - Box + Verb + TTC AP\n",
    "            - Box + Noun mAP\n",
    "            - Box + Noun + Verb mAP\n",
    "            - Box + Noun + TTC mAP\n",
    "            - Box + Noun + Verb + TTC mAP\n",
    "        :param iou_threshold: IOU threshold to check if a predicted box can be matched to a ground turth box\n",
    "        :param ttc_threshold: TTC threshold to check if a predicted TTC is acceptable\n",
    "        :param top_k: Top-K criterion for mAP. Discounts up to k-1 high scoring false positives\n",
    "        :param count_all_classes: whether to also average across classes with no annotations. False is the default for many implementations.\n",
    "        \"\"\"\n",
    "        super().__init__(12, top_k=top_k, count_all_classes=count_all_classes)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.tti_threshold = ttc_threshold\n",
    "\n",
    "        self.names = ['Box AP',\n",
    "                      'Box + Noun AP',\n",
    "                      'Box + Verb AP',\n",
    "                      'Box + TTC AP',\n",
    "                      'Box + Noun + Verb AP',\n",
    "                      'Box + Noun + TTC AP',\n",
    "                      'Box + Verb + TTC AP',\n",
    "                      'Box + Noun + Verb + TTC AP',\n",
    "                      'Box + Noun mAP',\n",
    "                      'Box + Noun + Verb mAP',\n",
    "                      'Box + Noun + TTC mAP',\n",
    "                      'Box + Noun + Verb + TTC mAP']\n",
    "\n",
    "        self.short_names = ['ap_box',\n",
    "                      'ap_box_noun',\n",
    "                      'ap_box_verb',\n",
    "                      'ap_box_ttc',\n",
    "                      'ap_box_noun_verb',\n",
    "                      'ap_box_noun_ttc',\n",
    "                      'ap_box_verb_ttc',\n",
    "                      'ap_box_noun_verb_ttc',\n",
    "                      'map_box_noun',\n",
    "                      'map_box_noun_verb',\n",
    "                      'map_box_noun_ttc',\n",
    "                      'map_box_noun_verb_ttc']\n",
    "\n",
    "    def _map_classes(self, preds):\n",
    "        \"\"\"\n",
    "        Associates each prediction to a class\n",
    "        :param preds: the input predictions\n",
    "        :return the matrix of classess associated to each prediction according to the evaluation measure\n",
    "        \"\"\"\n",
    "        nouns = preds['nouns']\n",
    "        ones = np.ones(len(preds['nouns']))\n",
    "\n",
    "        return np.vstack([\n",
    "            ones, # ap_box - do not average\n",
    "            ones, # ap_box_noun - do not average\n",
    "            ones, # ap_box_verb - do not average\n",
    "            ones, # ap_box_ttc - do not average\n",
    "            ones, # ap_box_noun_verb - do not average\n",
    "            ones, # ap_box_noun_ttc - do not average\n",
    "            ones, # ap_box_verb_ttc - do not average\n",
    "            ones, # ap_box_noun_verb_ttc - do not average\n",
    "            nouns, # map_box_noun - average over nouns\n",
    "            nouns, # map_box_noun_verb - average over nouns\n",
    "            nouns, # map_box_noun_ttc - average over nouns\n",
    "            nouns # map_box_noun_verb_ttc - average over nouns\n",
    "        ]).T\n",
    "\n",
    "    def _match(self, pred, gt_predictions, ious):\n",
    "        \"\"\"\n",
    "        Return matches of a given prediction to a set of GT predictions\n",
    "        :param pred: the prediction dictionary\n",
    "        :param gt_predictions: the gt predictions dictionary\n",
    "        :param ious: the computed IOU matrix (NGT x NPRED)\n",
    "        :return: a num_preds x num_ap matrix specifying possible matchings depending on the prediction and metric\n",
    "        \"\"\"\n",
    "        nouns = (pred['nouns'] == gt_predictions['nouns'])\n",
    "        boxes = (ious.ravel() > self.iou_threshold)\n",
    "        verbs = (pred['verbs'] == gt_predictions['verbs'])\n",
    "        ttcs = (np.abs(pred['ttcs'] - gt_predictions['ttcs']) <= self.tti_threshold)\n",
    "\n",
    "        tp_box = boxes\n",
    "        tp_box_noun = boxes & nouns\n",
    "        tp_box_verb = boxes & verbs\n",
    "        tp_box_ttc = boxes & ttcs\n",
    "        tp_box_noun_verb = boxes & verbs & nouns\n",
    "        tp_box_noun_ttc = boxes & nouns & ttcs\n",
    "        tp_box_verb_ttc = boxes & verbs & ttcs\n",
    "        tp_box_noun_verb_ttc = boxes & verbs & nouns & ttcs\n",
    "        # breakpoint()\n",
    "\n",
    "        return np.vstack([tp_box,  # ap_box\n",
    "                          tp_box_noun,  # ap_box_noun\n",
    "                          tp_box_verb,  # ap_box_verb\n",
    "                          tp_box_ttc,  # ap_box_ttc\n",
    "                          tp_box_noun_verb,  # ap_box_noun_verb\n",
    "                          tp_box_noun_ttc,  # ap_box_noun_ttc\n",
    "                          tp_box_verb_ttc,  # ap_box_verb_ttc\n",
    "                          tp_box_noun_verb_ttc,  # ap_box_noun_verb_ttc\n",
    "                          tp_box_noun,  # map_box_noun\n",
    "                          tp_box_noun_verb,  # map_box_noun_verb\n",
    "                          tp_box_noun_ttc,  # map_box_noun_ttc\n",
    "                          tp_box_noun_verb_ttc  # map_box_noun_verb_ttc\n",
    "                          ]).T\n",
    "\n",
    "class STAMeanAveragePrecision(AbstractMeanAveragePrecision):\n",
    "    \"\"\"Compute the different STA metrics based on mAP\"\"\"\n",
    "    def __init__(self, iou_threshold=0.5, ttc_threshold=0.25, top_k=5, count_all_classes=False):\n",
    "        \"\"\"\n",
    "        Construct the overall mAP metric. This will compute the following metrics:\n",
    "            - Box + Noun mAP\n",
    "            - Box + Noun + Verb mAP\n",
    "            - Box + Noun + TTC mAP\n",
    "            - Box + Noun + Verb + TTC mAP\n",
    "        :param iou_threshold: IOU threshold to check if a predicted box can be matched to a ground turth box\n",
    "        :param ttc_threshold: TTC threshold to check if a predicted TTC is acceptable\n",
    "        :param top_k: Top-K criterion for mAP. Discounts up to k-1 high scoring false positives\n",
    "        :param count_all_classes: whether to also average across classes with no annotations. False is the default for many implementations.\n",
    "        \"\"\"\n",
    "        super().__init__(4, top_k=top_k, count_all_classes=count_all_classes)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.tti_threshold = ttc_threshold\n",
    "\n",
    "        self.names = ['Box + Noun mAP',\n",
    "                      'Box + Noun + Verb mAP',\n",
    "                      'Box + Noun + TTC mAP',\n",
    "                      'Box + Noun + Verb + TTC mAP']\n",
    "\n",
    "        self.short_names = ['map_box_noun',\n",
    "                            'map_box_noun_verb',\n",
    "                            'map_box_noun_ttc',\n",
    "                            'map_box_noun_verb_ttc']\n",
    "\n",
    "    def _map_classes(self, preds):\n",
    "        \"\"\"\n",
    "        Associates each prediction to a class\n",
    "        :param preds: the input predictions\n",
    "        :return the matrix of classess associated to each prediction according to the evaluation measure\n",
    "        \"\"\"\n",
    "        nouns = preds['nouns']\n",
    "\n",
    "        return np.vstack([\n",
    "            nouns, # map_box_noun - average over nouns\n",
    "            nouns, # map_box_noun_verb - average over nouns\n",
    "            nouns, # map_box_noun_ttc - average over nouns\n",
    "            nouns # map_box_noun_verb_ttc - average over nouns\n",
    "        ]).T\n",
    "\n",
    "    def _match(self, pred, gt_predictions, ious):\n",
    "        \"\"\"\n",
    "        Return matches of a given prediction to a set of GT predictions\n",
    "        :param pred: the prediction dictionary\n",
    "        :param gt_predictions: the gt predictions dictionary\n",
    "        :param ious: the computed IOU matrix (NGT x NPRED)\n",
    "        :return: a num_preds x num_ap matrix specifying possible matchings depending on the prediction and metric\n",
    "        \"\"\"\n",
    "        nouns = (pred['nouns'] == gt_predictions['nouns'])\n",
    "        boxes = (ious.ravel() > self.iou_threshold)\n",
    "        verbs = (pred['verbs'] == gt_predictions['verbs'])\n",
    "        ttcs = (np.abs(pred['ttcs'] - gt_predictions['ttcs']) <= self.tti_threshold)\n",
    "\n",
    "        tp_box_noun = boxes & nouns\n",
    "        tp_box_noun_verb = boxes & verbs & nouns\n",
    "        tp_box_noun_ttc = boxes & nouns & ttcs\n",
    "        tp_box_noun_verb_ttc = boxes & verbs & nouns & ttcs\n",
    "        # breakpoint()\n",
    "        return np.vstack([tp_box_noun,  # map_box_noun\n",
    "                          tp_box_noun_verb,  # map_box_noun_verb\n",
    "                          tp_box_noun_ttc,  # map_box_noun_ttc\n",
    "                          tp_box_noun_verb_ttc  # map_box_noun_verb_ttc\n",
    "                          ]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cell has been hidden away (https://colab.research.google.com/drive/1Ok_6F1O6K8kX1S4sEnU62HoOBw_CPngR?usp=sharing#scrollTo=gY9AG6c-6NF4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model On Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(path_to_save)\n",
    "metric_map_val, preds_val = evaluate(sta_val, val_video_uids_to_use, val_feature_path, 256, 2, best_model, pred_object_threshold, next_active_threshold, False)\n",
    "metric_map_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_map_val, preds_val = evaluate(sta_val, val_video_uids_to_use, val_feature_path, 256, 2, best_model, pred_object_threshold, next_active_threshold, False)\n",
    "metric_map_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = pred_to_dict(preds_val, sta_val)\n",
    "json.dump(pred_dict, open(\"/content/val_pred.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_dict[\"results\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results are aligned with val set\n",
    "\n",
    "%%bash\n",
    "cd forecasting\n",
    "\n",
    "VAL_RES_JSON=\"/content/val_pred.json\"\n",
    "ANN_FILE=\"/content/ego4d_data/v1/annotations/fho_sta_val.json\"\n",
    "\n",
    "export PYTHONPATH=\"./\"\n",
    "python3 tools/short_term_anticipation/evaluate_short_term_anticipation_results.py $VAL_RES_JSON $ANN_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map, preds = evaluate(sta_test, test_video_uids_to_use, test_feature_path, 256, 2, best_model, pred_object_threshold, next_active_threshold, False)\n",
    "metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = pred_to_dict(preds, sta_test)\n",
    "json.dump(pred_dict, open(\"/content/pred_results_test.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When submitting the JSON file - you can expect results on the order of:\n",
    "# {\"Noun\": 20.450272621964917, \"Noun_Verb\": 4.812352890197652, \"Noun_TTC\": 4.3974579149093715, \"Overall\": 1.3109038962917996}\n",
    "# map_box_noun_verb_ttc at ~1.78 for the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
