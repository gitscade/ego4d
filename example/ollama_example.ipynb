{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "import requests\n",
    "import json\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ollama 서버가 백그라운드에서 실행되었습니다.\n",
      "LLM stands for Large Language Model. It refers to a type of artificial intelligence (AI) model that is trained on vast amounts of text data and can generate human-like language outputs.\n",
      "\n",
      "Large Language Models are designed to process and understand natural language, such as text or speech, in a way that is similar to how humans do. They are capable of learning patterns and relationships within the language, which enables them to generate coherent and relevant text responses.\n",
      "\n",
      "Some key characteristics of LLMs include:\n",
      "\n",
      "1. **Scale**: LLMs are trained on massive datasets, often comprising billions of words or more.\n",
      "2. **Self-supervised learning**: LLMs learn from themselves, without explicit human supervision, by predicting the next word in a sequence.\n",
      "3. **Generative capabilities**: They can generate new text that is coherent and relevant to a given topic or context.\n",
      "4. **Contextual understanding**: LLMs can understand the nuances of language, including idioms, sarcasm, and figurative language.\n",
      "\n",
      "LLMs have numerous applications across various industries, such as:\n",
      "\n",
      "1. **Natural Language Processing (NLP)**: They are used in NLP tasks like text classification, sentiment analysis, and machine translation.\n",
      "2. **Chatbots**: LLMs power conversational AI systems that can engage with humans in natural-sounding conversations.\n",
      "3. **Content generation**: They can generate high-quality content, such as articles, product descriptions, or social media posts.\n",
      "4. **Language understanding**: LLMs are used in language-related tasks like text summarization, question answering, and machine learning.\n",
      "\n",
      "Examples of popular LLMs include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "3. XLNet\n",
      "4. Transformer-XL\n",
      "\n",
      "These models have significantly improved the state-of-the-art in NLP and are driving innovation in various industries, from customer service to content creation.\n"
     ]
    }
   ],
   "source": [
    "def start_ollama_server():\n",
    "    \"\"\"\n",
    "    Ollama 서버를 백그라운드에서 실행하여 llava:7b 모델을 로드.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ollama 서버를 백그라운드에서 실행\n",
    "        subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        time.sleep(2)  # 서버가 시작될 시간을 확보\n",
    "        print(\"[INFO] Ollama 서버가 백그라운드에서 실행되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama 서버 실행 실패: {e}\")\n",
    "\n",
    "start_ollama_server()\n",
    "\n",
    "# Initialize Ollama with your chosen model\n",
    "llm = Ollama(model=\"llama3:8b\")\n",
    "\n",
    "# Invoke the model with a query\n",
    "response = llm.invoke(\"What is LLM?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for \"Language Model.\" It's a type of artificial intelligence that has the ability to understand and generate human-like language based on statistical patterns in large amounts of data. Language models are commonly used in natural language processing tasks, such as text generation, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama with your chosen model\n",
    "llm = Ollama(model=\"llava:34b\")\n",
    "\n",
    "# Invoke the model with a query\n",
    "response = llm.invoke(\"What is LLM?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Image to Base64 and inquiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llava:34b', 'created_at': '2025-02-18T07:07:52.811658555Z', 'response': \"The image shows a person preparing food, specifically what appears to be a mixed salad or vegetable stir-fry. There are various ingredients like onions and what looks like bell peppers being chopped on a cutting board. In the background, there's a kitchen counter with some condiments and a bottle that might be an oil or vinegar for cooking. The setting suggests someone is in the process of preparing a meal.\", 'done': True, 'done_reason': 'stop', 'context': [1581, 59705, 622, 59593, 5858, 46826, 10707, 144, 7, 59568, 144, 59666, 59705, 622, 59593, 5858, 46826, 3903, 144, 59653, 5947, 59594, 77, 59651, 144, 144, 5697, 620, 594, 719, 2728, 100, 7, 59568, 144, 59666, 59705, 622, 59593, 5858, 46826, 765, 13611, 144, 1263, 2728, 2587, 562, 1514, 14194, 2465, 97, 7506, 981, 5405, 592, 629, 562, 8645, 21022, 705, 26219, 15131, 59594, 59585, 851, 98, 1889, 678, 3225, 10125, 947, 30214, 597, 981, 3689, 947, 23291, 39893, 1325, 29271, 632, 562, 10101, 4165, 98, 967, 567, 4385, 97, 926, 59610, 59575, 562, 7421, 5931, 651, 919, 1925, 5304, 597, 562, 10709, 639, 1771, 629, 663, 4248, 705, 31841, 631, 10777, 98, 707, 4751, 8441, 2491, 620, 594, 567, 1621, 593, 14194, 562, 10384, 98], 'total_duration': 7857314252, 'load_duration': 5393739492, 'prompt_eval_count': 616, 'prompt_eval_duration': 446000000, 'eval_count': 83, 'eval_duration': 2016000000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# Set up Ollama API URL (change port if needed)\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"  # Change if running in Docker (e.g., 2222)\n",
    "\n",
    "# Convert image to Base64\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Provide the image path\n",
    "image_path = \"example1.jpg\"  # Replace with your actual image path\n",
    "image_base64 = encode_image(image_path)\n",
    "\n",
    "# Prepare the request payload\n",
    "data = {\n",
    "    \"model\": \"llava:34b\",  # Ensure this model is installed via `ollama pull lava:34b`\n",
    "    \"prompt\": \"What is in this image?\",\n",
    "    \"images\": [image_base64], \n",
    "    \"stream\": False # True returns chunks in real time rather than whole answer\n",
    "}\n",
    "\n",
    "# Example of data payload with other options\n",
    "# data = {\n",
    "#     \"model\": \"llava:34b\",\n",
    "#     \"prompt\": \"Describe the objects in this image.\",\n",
    "#     \"images\": [image_base64],  # Base64-encoded image\n",
    "#     \"stream\": False,\n",
    "#     \"format\": \"json\",  # Structured output\n",
    "#     \"options\": {\n",
    "#         \"temperature\": 0.7,  # Higher values = more creative responses\n",
    "#         \"top_k\": 50,  # Limit the number of token choices\n",
    "#         \"top_p\": 0.9  # Control nucleus sampling\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(OLLAMA_URL, json=data)\n",
    "\n",
    "# Print response\n",
    "#print(response.json())\n",
    "#print(response.json().get(\"created_at\", \"\"))\n",
    "print(response.json().get(\"response\", \"\")) # only the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18T07:07:52.811658555Z\n",
      "The image shows a person preparing food, specifically what appears to be a mixed salad or vegetable stir-fry. There are various ingredients like onions and what looks like bell peppers being chopped on a cutting board. In the background, there's a kitchen counter with some condiments and a bottle that might be an oil or vinegar for cooking. The setting suggests someone is in the process of preparing a meal.\n"
     ]
    }
   ],
   "source": [
    "print(response.json().get(\"created_at\", \"\"))\n",
    "print(response.json().get(\"response\", \"\")) # only the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
