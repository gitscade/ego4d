{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG that predicts goal based on current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# parser to extract string from answer\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete keys before commiting to github\n",
    "OPENAI_API_KEY = \"YOUR KEY\"\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = 'YOUR KEY'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking model like this\n",
    "- using parser to get string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# prompt -> model -> parser structure\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing Json Dataset into a database\n",
    "\n",
    "- We will trascribe a json dataset into a database\n",
    "- We will then use search query to find relevant context based on the query\n",
    "- The query will give information on current structure of the environment.\n",
    "- The quesry will also ask the overlaying goal of the current user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding a query\n",
    "\n",
    "- embed the querly using OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "queryToEmbed = \"What is the goal of the user?\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(queryToEmbed)\n",
    "\n",
    "# len of the embedding will be set as the lenght set in the browser interface\n",
    "print(f\"Embedding length: {len(embedded_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VectorStore to make a dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"ddd\",\n",
    "        \"fff\",\n",
    "        \"ggg\"\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT / JSON splitting to Documents\n",
    "### B-1 TEXT Loading and Splitting\n",
    "1. load transcription\n",
    "2. load transcription into loader (txt->memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE in loading\n",
    "with open(\"transciption.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading transcription, use use loader instead of reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents\n",
    "\n",
    "# JUST use default splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using json, we can use CharacterTextSplitter. However, this can kill context in a json file. How about that we use splitters that can recognize arrays or key-value pairs, and split json while respecting the file's structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-2 JSON: use RecursiveJsonSplitter\n",
    "- https://python.langchain.com/docs/how_to/recursive_json_splitter/\n",
    "- use `RecursiveJsonSplitter` & `.split_json `\n",
    "\n",
    "Most chatgpt examples after the second question use CharacterTextSplitter or derivations from it. Maybe not the best use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# load and json splitter\n",
    "with open(\"myjson.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# OPT1: GET CHUNKS\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks= json_splitter.split_json(json_data=json_data)\n",
    "\n",
    "# OPT2: GET DOCUMENTS\n",
    "docs = json_splitter.create_documents(texts=[json_data])\n",
    "for doc in docs[:3]:\n",
    "    print(doc)\n",
    "\n",
    "# OPT3: GET TEXTS\n",
    "texts = json_splitter.split_text(json_data=json_data)\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B-2 JSON: Other recommended methods\n",
    "- https://chatgpt.com/c/673fe115-4d18-8007-beb2-027fd355216e\n",
    "- see answer after the first question. These answers give insight, but not the most appropriate solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use ijson: STREAM & Process to chunks for large json file. Not the most suitable example\n",
    "# import ijson\n",
    "# with open('large_file.json', 'r') as f:\n",
    "#     parser = ijson.items(f, 'items')  # Adjust 'items' based on your JSON structure\n",
    "#     for item in parser:\n",
    "#         print(item)\n",
    "\n",
    "# # split based on array elements: RecursiveJsonSplitter can do similar thinks I guess\n",
    "# import json\n",
    "\n",
    "# with open('large_file.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Assume the large array is under 'data'\n",
    "# chunk_size = 100\n",
    "# chunks = [data['data'][i:i + chunk_size] for i in range(0, len(data['data']), chunk_size)]\n",
    "\n",
    "# # Save each chunk to a separate file\n",
    "# for idx, chunk in enumerate(chunks):\n",
    "#     chunk_data = {'data': chunk}  # Re-wrap in a JSON structure if needed\n",
    "#     with open(f'chunk_{idx}.json', 'w') as f:\n",
    "#         json.dump(chunk_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Configuring multiple dataset stores\n",
    "\n",
    "- C-1: embed separately and merge to single dataset\n",
    "- C-2: separate embeddings and search in parallel using langchain's `MultiRetriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
