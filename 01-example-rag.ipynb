{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag Structure for Inference\n",
    "\n",
    "https://github.com/svpino/youtube-rag/blob/main/rag.ipynb\n",
    "\n",
    "We are using a youtube video to \n",
    "- make a rag dataset\n",
    "- infer from queries using langchain\n",
    "\n",
    "use whatever models that you want\n",
    "- 3.5 turbo\n",
    "- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# delete keys before commiting to github\n",
    "OPENAI_API_KEY = \"YOUR KEY\"\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = 'YOUR KEY'\n",
    "\n",
    "\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\" # THIS VIDEO is 3hr+ long!\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=ZmtvsX7aWzo\" # 1:48hr. Reiner knizia 100 games\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=O6C66NFCkJ8\" # 5min. creator on monetization\n",
    "# YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=SHNAw80wbUs\" #10min korean video. hard to get\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=SBGG4WNweEc\" #nobel prize in physics\n",
    "\n",
    "# test the model\n",
    "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we only want to get script, let's get the answer straight from the parser\n",
    "`StrOutputParser` extracts answer as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Los Angeles Dodgers won the World Series during the COVID-19 pandemic in 2020.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# instead of invoking model, we will invoke the model parser to get string directly (query-->model-->response-->parser-->result)\n",
    "chain = model | parser\n",
    "chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing prompt templates\n",
    "We want to provide the model with some context and the question. Prompt templates are a simple way to define and reuse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the prompt with the model and the output parser. Which means:\n",
    "\n",
    "We chain a prompt like this:\n",
    "prompt = context + question\n",
    "\n",
    "and invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Susana'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining chains\n",
    "We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.\n",
    "\n",
    "1) First chain: invoke the first chain to get paresed response\n",
    "2) Second chain: first answer is used in the second chain's prompt, along with new context\n",
    "\n",
    "Let's start by creating a new prompt template for the translation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new translation chain that combines the result from the first chain with the translation prompt.\n",
    "\n",
    "Here is what the new workflow looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'María tiene una hermana, Susana.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE: Transcribing the YOUTUBE VIDEO\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using OpenAI's Whisper.\n",
    "\n",
    "youtube video ==> text script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ↳ |████████████████████████████████████████████| 100.0%\r"
     ]
    }
   ],
   "source": [
    "#install whisper with thie command : pip install git+https://github.com/openai/whisper.git \n",
    "#install ffmpeg : apt instll ffmpeg or use sudo\n",
    "# If there is an error, use pytubefix. MAYBE youtube api change affected pytube's performance (still issue in 2024.11)\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytubefix import YouTube # does not work\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "\n",
    "# if not os.path.exists(\"transcription.txt\"):\n",
    "#     youtube = YouTube(YOUTUBE_VIDEO)\n",
    "#     audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "#     # Let's load the base model. This is not the most accurate\n",
    "#     # model but it's fast.\n",
    "#     whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         file = audio.download(output_path=tmpdir)\n",
    "#         transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "#         with open(\"transcription.txt\", \"w\") as file:\n",
    "#             file.write(transcription)\n",
    "\n",
    "\n",
    "# Let's do this only if we haven't created the transcription file yet.\n",
    "# We already defined \"YOUTUBE_VIDEO\" at the starting cell\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO, on_progress_callback = on_progress)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # Let's load the base model. This is not the most accurate\n",
    "    # model but it's fast.\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first 100 characters in the trascription text to check if transcription is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Välkommen till Kungliga vetenskap Sakademin och när presskonferensen då vi ska presentera årets Nobe'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the entire transcription as context\n",
    "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
    "\n",
    "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate error code because prompt is simply too big!\n",
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is reading papers a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the transcription\n",
    "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:\n",
    "\n",
    "Let's start by loading the transcription in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Välkommen till Kungliga vetenskap Sakademin och när presskonferensen då vi ska presentera årets Nobelpris i fysisk. Välkomna till presskonferens och Royal Swedish Academy of Sciences. Vad vi vill presentar är Nobelpris i fysisk. Vi vill kippa till vår tradition och börja med presentation i Swedish och då kan vi gå in i English. Och du är välkomna att få en fråga i en länge som ska gå upp. Jag heter Hans Ellegren och är ständig sektera här på Kungliga vetenskapsakademin. Till höger om mig sitter professor Ellen Wons, ordförande i Nobelkomitén för fysisk. Mot i vänster sitter professor Anders Irbeck, leda mot de välkomitén för fysisk och expert inom älnesområdet. My name is Hans Ellegren, under Secretary General of the Royal Swedish Academy of Sciences. And to my right is professor Ellen Wons, chair of the Nobelkomitén för fysisk. And to my left is professor Anders Irbeck, member of the Nobelkomitén för fysisk och expert inom fysisk. Åres pris handlar om masiner som läser. This year's price is about machines that learn. Kungliga vetenskapsakademin har beslutat att utdelar 20-24 års nobeltis i fysisk till John Hopfield, Princeton University USA och Jeffrey Hinton University of Toronto Canada. För grundlängande upptäckte raka uppfiningar så möjliggör machine inlärning och artificiella neroan nätverk. The Royal Swedish Academy of Sciences has today decided to award a 20-24 Nobel Prize in fysisk to John Hopfield, Princeton University USA och Jeffrey Hinton University of Toronto Canada, för fundational diskarbris och inventions att enable machine learning med artificial neroan nätverk. Professor Ellen Wons, vi nu har ett särskilt samarbete. Thank you. Learning is a fascinating ability of the human brain. We can recognize images and speech and associate them with memories and past experiences. Billions of neurons wire together give us unique cognitive abilities. Artificial neural networks are inspired by this network of neurons in our brains. This year's laureates for the Nobel Prize in physics, John Hopfield and Jeffrey Hinton used fundamental concepts from statistical physics to design artificial neural networks that function as associative memories and find patterns in large data sets. These artificial neural networks have been used to advance research across physics topics as diverse as particle physics, material science, and astrophysics. They have also become part of our daily lives, for instance in facial recognition and language translation. The laureates, discoveries, and inventions form the building blocks of machine learning that can aid humans in making faster and more reliable decisions, for instance when diagnosing medical conditions. However, while machine learning has enormous benefits, its rapid development has also raised concerns about our future. Collectively humans carry the responsibility for using this new technology in a safe and ethical way, for the greatest benefit of humankind. Thank you. Professor Ebeck, are you ready to give a more detailed presentation? Thank you. Please. So this year's Nobel Prize in physics is about our artificial neural networks. Today we know that this is a powerful computational approach. This was not evident 50 years ago, but it was known that we mammals are very good at pattern recognition by some sort of computation in our brains. And this sparked an interest in understanding the collective properties of networks of simplified neurons, connected by couplings with a strength that could become weaker or stronger. And the idea would then be to determine the strengths of the couplings to achieve a certain function, and doing that by training the network on many examples. A breakthrough came in 1982, when John Hopfield presented a dynamic network, which could store and retrieve memories, an associative memory. The memory had simple binary zero-one nodes, all nodes connected, parabys connected. The states that remained unchanged with time were identified as memories. Moreover, it was possible to introduce an energy similar to an energy one has in studying magnetic systems in physics. And that energy had the property that it was low in the state's corresponding to memories. Metaphorically, the memories were located in the list of energy landscape. When starting from this stupid pattern with a higher energy, it would slide down the network, would slide down in energy to a nearby valley. And by this process, this stupid pattern could be corrected. In follow-up work, John Hopfield also showed that this network was robust in the sense that the binary nodes could be replaced with analog ones, and he also showed how the network could be used to solve difficult optimization problems. The creation and explorations of this network by John Hopfield was milestone in our understanding of the computational abilities of artificial neural networks. Another important discovery came soon afterwards by Joffrey Hinton and Terence Seynopski. They created a stochastic version of the Hopfield network based on statistical physics and called the Boltzmann machine. So here, the focus is on statistical distributions of patterns rather than individual patterns. It is generative model. Once trained, it can be used to generate new instances from the learned distribution. It had the same basic structure as Hopfield's network, but there were two types of nodes, hidden and visible ones, and the hidden nodes were there to make it possible for the network to learn more generative distributions. While theoretically interesting, in practice, the Boltzmann machine was initially of limited use. It was prohibitively demanding computationally. However, a version with fewer couplings, called the restrict Boltzmann machine, developed into a versatile tool, and Icon will soon mention it again. So far, I talked about recurrent networks with feedback connections. Many of today's deep learning methods involved feedback networks, where information flow from an input layer to an output layer via hidden layers. The 1980s hint on how such a network with hidden layers could be trained, and in that process, he also elucidated the important function of hidden layers. Then in the 1990s, there were applications of multi-layer networks, successful applications, for example, for the classification of handwritten digits, but the networks that one called Train had relatively few couplings between consecutive layers. It remained a challenge to train more general deep structures with high connectivity between the layers. Here, in fact, many gave up, but hint on did not. And hint on, overcame this barrier by using this restricted Boltzmann machine. He used it to pre-train deep structures, and by this method, he succeeded in implementing examples of deep and dense structures. Which was a breakthrough toward deep learning. Finally, so now I have talked about physics, how physics has been a driving force in innovation and development. It's also interesting to see how physics as a research field is benefiting from these methods. One very established example here is data analysis in particle physics and astrophysics. An increasingly important application is in modeling materials. For example, to search for more efficient solar cells. Yet another example is in explicit physics-based climate modeling to enable higher resolution. Finally, I want to mention two applications, successful applications, outside the physics area, protein structure prediction, and analysis of medical images. Thank you for your attention. Thank you, Professor Irbeck. I think we might have John Hoppfield, or Jeff Hinton, with us on the phone. Good morning, Professor Hinton. Good morning. Please accept our warmest congratulations to receiving the Nobel Prize in physics. Thank you very much. How do you feel right now? I'm flabbergasted. I had no idea this would happen. I'm very surprised. I could imagine. I'm sitting here in this beautiful session hall of the Royal Swedish Academy of Sciences. Here at the press conference, there are many interested journalists from both the Swedish and the international press. Would you be ready to take some questions from them? Yes. Yes, please. Sorry, is there anyone? Thank you. My warmest congratulations to your achievements. And to this year's Nobel Prize in physics. My name is Susan Ritzin. And my question comes from Swedish television. I know many of our viewers, also lay people are very curious about the discovery of the word it here today. I wonder, do you remember when you realized your breakthrough, awarded today, if you can bring us back in time briefly? And what were the reasons for, or the inspiration for these revelations? So I remember a couple of occasions with two of my mentors. So I have an enormous debt to David Rommelhardt and Teresyn Offsky. With David Rommelhardt, we discovered the back propagation algorithm. And that within the beginning of 1982. And with Teresyn Offsky, Teri and I discovered a learning algorithm for hotfield nets that had hidden units. And I remember very well going to a meeting in Rochester where John Hopfield talked. And we, I first learned about the hotfield energy function for neural networks. And after that, Teri and I worked feverishly on how to generalize neural networks to have hidden units. And at the beginning of 1982, we came up with a learning algorithm for ultimate machines, which are hotfield nets with hidden units. So the most exciting times were with David Rommelhardt and back propagation in Teresyn Offsky on both of the machines. Thank you. Okay. More questions? First here. Hello, Bogin Radecki from the Polish Television. Congratulations. The question I have is a little bit about the future because obviously we are very excited about what neural networks and machine learning can do now. But what we're even more excited is the prospect of what they could do in the future. What are your predictions about the degree of influence that this technology is going to have on our civilization? I think it will have a huge influence. It will be comparable with the industrial revolution. But instead of exceeding people in physical strength, it's going to exceed people in intellectual ability. We have no experience of what it's like to have things smarter than us. And it's going to be wonderful in many respects. In areas like healthcare, it's going to give us much better healthcare. In almost all interest, it's going to make them more efficient. People are going to be able to do the same right to work with an AI assistant in much less time. It'll mean huge improvements in productivity. But we also have to worry about a number of possible bad consequences, particularly the threat of these things getting out of control. I think first we have here and then there. Hi, I'm Stelimon Kampanello with the audience. Congratulations on the price. My question is, last year you set in an interview with New York Times that you regret part of your life's work because of their risks with artificial intelligence. How do you feel about today? There's two kinds of regrets. There's regrets where you feel guilty because you did something you shouldn't have done. And then there's regret where you did something that you would do again in the same circumstances. But it may in the end not turn out well. That second kind of regret I have. In the same circumstances, I would do the same again. But I am worried that the overall consequence of this might be systems more intelligent than us that eventually take control. Yes, please. Hi, congratulations on the price. My name is Almyn Niedner. I'm from TV4, Swedish TV Channel. I would like to develop this old smart machine. And I wonder what's the time of AI? What type of AI do you... Has come out of it like a GPT or how you see breast cancer in X-rays or how you make funny pictures in Dali. What kind of AI do your research? Do build on your research? So there were two different learning algorithms I was involved in. One was the Boltzmann machine, which was a learning algorithm for popular networks with hidden units. And we did eventually find a practical version of that. But that's not what's led to the main progress currently in your networks. That's the back propagation algorithm. And this is a way to get a neural net to learn anything. And it's the back propagation algorithm that's led to the huge surge in AI applications and in the ability to recognize images and understand speech and to deal with natural language. It's not the Boltzmann machine that did that. It's the back propagation algorithm. More questions? One here please. Hi, my name is Bill. I'm from the Swedish paper and a technique. Do you have any favorite AI tool that you use? I actually use GPT-4 quite a lot. Whenever I want to know the answer to anything, I just go and ask GPT-4. I don't totally trust it because it can hallucinate. But on almost everything, it's a not very good expert. And that's very useful. Okay. Don't see any more. Is there one more hand there? Please. Yes, hello. Congratulations, Paul Reese from Al Jazeera English. Could you just give us a sense of where you were when you got the call? How it affected you? Is this the day you have in your diary just in case you get that call or is it a bolt from the blue? It was a bolt from the blue. It's a cheap hotel in California that doesn't have an internet connection and doesn't have a very good phone connection. I was going to get an MRI scan today, but I think I'll have to cancel that. Okay, this seems to be the last question from the press for you, Professor Hinton. Thank you. Thank you. And once again, our warmest congratulations. We look forward to seeing you here in Stockholm in December for the Nobel Prize ceremony. Thank you. Okay. So let's move on to more questions about the physics prize and the research involved. Or if you want to ask the committee members questions about their work. Questions are welcome in either English or Swedish. Please. Yes. If these two scientists wouldn't have existed, would we have like GPT then? Professor Mons, do you want to address that? That's a very good difficult question to answer because it's hard to imagine. They have contributed, of course, enormously, very early in the progress of this technology. So in the 80s, these first steps were taken. And later on, other scientists have built upon these developments. So in a sense, it may have been difficult without those groundbreaking first discoveries and inventions. I'm thinking since John Jay Hopfield is not here, I was wondering a little bit about what makes you most... What do you think is the most exciting part of his discoveries? Professor Ibeck, do you want to rest at? When it comes to his network, he was... Parts of it had been discussed earlier, but he was able to put pieces together to have created a network with a clear function and clear principles for how it worked. And it meant a lot in the field. One more question here? Oh, sorry, yes. I also wonder about the worries that Jeffrey Hinton expressed. What are your worries about this technology? Professor Mons, do you want to rest? Well, these type of worries are expressed a lot and discussed in the scientific community. And I think it is very good that they are discussed and it contributes to the knowledge about machine learning in the society. I think it is important that as many people as possible learn about the mechanisms of machine learning so that it is not just in the hands of a few individuals. So I think, of course, Professor Hinton is one of many who express their views on this. I think that is very good. Maybe I can add that there are, of course, many discoveries and inventions over the time that has been potentially possible to misuse, but it is a common responsibility for a society to have regulations, to avoid that to happen. And I think that would apply to artificial intelligence. I think that is it, actually, time is running out. Thank you for your interest in participating in this press conference. We hope to see you again here tomorrow when we will present the Nobel Prize in Chemistry. So thank you. Thank you. Thank you. How can I make it to the other side? I think it is very important. Yes. I was right. I was right. I was right. I don't know. I don't know. I saw him just now. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I think he is a very talented person. I always would like premier. Hello tea. I always wanted to see her do goodม. This together has been a dem Good moment. Hello tea. Yes girls, everyone will drink this beer. Hello tea, everyone will drink this beer. – Jag driver Bavlöda S judiciaryÅ innebär bil för att inkomligen manuskruppen som har en program med dig Frcild av idées och methods från Finland. Var ska vi ta tusen en kvinna vad det nesse som blir tilllor i vår respekt? Ja, i några mytheіл i odpowied controversy Finland, när jag ska flaggrava flagangen varayım om för att den värde brukar vi tala med ouran Oops Varing inte, när den kommer och får Nural Networks. I kredit. Modern. With element som vitt var similar to what one has in magnetic models in physics, but this was a new thing for neural networks. And it was It was good because it put together different elements. It was, he gave the network a clear function and it worked after, according to clear principles. All right. And created what part was the sort of the memory part? Yes, yes, this was the associative memory. Exactly. Yes. And the other one already? Ej, jefer Hinton. Jämt. Very soon afterwards he created a model directly based on, it based on the hope for network, but he changed so that the focus was now not on individual memories individual patterns, but on statistical distributions of patterns. That was one thing in the 80s by Jeff Hinton. He also created a learning algorithm for what is called Fried multilayered networks, Friedforward multilayered networks. What could that do? Ej, so the whole idea in artificial neural network is that I have my system of nodes or neurons. They are connected by couplings of and those can have different strengths. And one that in order to achieve some function, one has to train these the network on many examples and training means that one tries to determine good values for these couplings. And this is complex because there are a lot of couplings in such a network. And he, Hinton created learning algorithm for this whole smart machine and for multinein layer, feed forward structures. And that worked through very important contributions. And these are some important contributions for what we today call AI artificial intelligence. Will you tell me something about how this I think we've all heard about it. But what would you say are the most important ways this affect us today? In many, many ways. So in not least in science, in physics and other scientific fields, it's a physics, these tools based on artificial network. They have been around for quite some time already before. It became deep learning. The Bermannian many use for tools. And now as the artificial neural network tools get more and more powerful, we steadily see new applications and material science, modeling in materials science is one important example. And outside physics, it was pointed out by Hinton himself, Indian, but certainly healthcare is very important. It is already a very good tool for analysis of medical images of different countries. Now we just heard Jeffrey Hinton here in the press conference saying he was flabbergasted. Is there anything personal you know about the laureates that you would like to share with us? That's very important to you, the same question. No, maybe not actually. I mean, personally, I wouldn't say no. No. They have both been really, I think, through pioneers. And finding new ways to tackle problems. How would you tell us in a sentence or two just when you are excited about awarding the price to this particular field this year? I think it is fantastic to create a completely new way of computing and see how it develops into such a powerful tool. Thank you. Thank you very much, Professor Amdistir Rik, member of the Nobel Physics Committee. Yep, thank you. You\")]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to split a document. For this example, we'll use a simple splitter that splits the document into chunks of a fixed size. Check Text Splitters for more information about different approaches to splitting documents.\n",
    "\n",
    "For illustration purposes, let's split the transcription into chunks of 100 characters with an overlap of 20 characters and display the first few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content='Välkommen till Kungliga vetenskap Sakademin och när presskonferensen då vi ska presentera årets'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='presentera årets Nobelpris i fysisk. Välkomna till presskonferens och Royal Swedish Academy of'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='Swedish Academy of Sciences. Vad vi vill presentar är Nobelpris i fysisk. Vi vill kippa till vår'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='vill kippa till vår tradition och börja med presentation i Swedish och då kan vi gå in i English.'),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content='vi gå in i English. Och du är välkomna att få en fråga i en länge som ska gå upp. Jag heter Hans')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our specific application, let's use 1000 characters instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the relevant chunks\n",
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of embeddings comes into play.\n",
    "\n",
    "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. You can use the Cohere's Embed Playground to visualize embeddings in two dimensions.\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[-0.0013731546932831407, -0.034482136368751526, -0.011498215608298779, 0.0012331805191934109, -0.0261743925511837, 0.009077209047973156, -0.015739668160676956, 0.0017250451492145658, -0.011854797601699829, -0.03325599059462547]\n"
     ]
    }
   ],
   "source": [
    "# embeddings from an arbitrary query\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
    "sentence2 = embeddings.embed_query(\"Pedro's mother is a teacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use Cosine Similarity to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9173394718348284, 0.7680513114191245)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Vector Store\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a vector store.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the vector store to find the most similar embeddings to a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       "  0.9173394801008797),\n",
       " (Document(metadata={}, page_content='Mary has two siblings'),\n",
       "  0.9044728659809682),\n",
       " (Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       "  0.8013463844876163)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the vector store to the chain\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
    "\n",
    "\n",
    "We need to configure a Retriever. The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       " Document(metadata={}, page_content='Mary has two siblings'),\n",
       " Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       " Document(metadata={}, page_content=\"Pedro's mother is a teacher\")]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='Patricia likes white cars'),\n",
       "  Document(metadata={}, page_content='Lucia drives an Audi'),\n",
       "  Document(metadata={}, page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(metadata={}, page_content=\"Mary's sister is Susana\")],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lucia drives an Audi.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading transcription into the vector store\n",
    "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hinton's contribution was creating a learning algorithm for multilayered neural networks, specifically for feedforward structures, which made important contributions to what we now call artificial intelligence (AI).\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"what was hinton's contribution?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Pinecone\n",
    "So far we've used an in-memory vector store. In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. For this example, we'll use Pinecone.\n",
    "\n",
    "The first step is to create a Pinecone account, set up an index, get an API key, and set it as an environment variable PINECONE_API_KEY.\n",
    "\n",
    "Then, we can load the transcription documents into Pinecone:\n",
    "\n",
    "Run the cell that uses the langchain approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This method is not compatible with Langchain\n",
    "## https://www.whatwant.com/entry/Vector-Database-Pinecone\n",
    "\n",
    "#import os\n",
    "#import pinecone\n",
    "\n",
    "## pinecone api (personal key)\n",
    "#PINECONE_API_KEY = ''\n",
    "#PINECONE_ENV = 'gcp-starter'\n",
    "\n",
    "#pinecone.init(api_key=PINECONE_API_KEY,\n",
    "#              environment=PINECONE_ENV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://docs.pinecone.io/integrations/langchain\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "# in my pinecone databse index, the openAI embedding model uses 1536 dim which can also be selected in the browser interface\n",
    "index_name = \"youtube-rag-index\"\n",
    "#OpenAIEmbeddings already assigned in prior cell\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c131602f-ad68-4f2f-9bec-dd3ef32b2302', metadata={'source': 'transcription.txt'}, page_content='사람들 뭐 한번 해드볼까 아꺼워 아꺼워 아꺼워 으음 뭐했다고? 아꺼워 뭐 없어? 아꺼워 베리야 아까 한 마디도 못하니 이렇게 시끄러워 어? 왜 이렇게 시끄러워? 베리야 뭐라고? 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워 아꺼워'),\n",
       " Document(id='03f74b36-09e9-4bea-8c6c-c977cdfbfff6', metadata={'source': 'transcription.txt'}, page_content='아, 잘하네 아, 잘하네 아, 잘하네 요리 안녕 해봐 서로 등도로 안녕하세요 베렛버입니다 여러분 베리가 말도 잘하고 노래도 잘하고 화도 잘 내고 그런 주만 하시죠 오늘 베리가 행무샤 친구를 만나러 갔는데 맞죠? 한마디도 못하고 잔만 찾아가고 왔니? 그렇게 시끄럽던 베리가 아, 깨서는 꿀먹어 방어리가 됩니다 오늘은 요리에 색다른 모습을 보시게 될 겁니다 몰라지 마세요 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 요리는 요리는 요리는 요리는 그만 그만 그만 그만 그만 그만 그만 그만 하라고 그만 요리는 요리는 요리는 어지러? 어지러 갈거야 요리 요리는 요리는 요리는 요리는 요리는 아, 잘하네 이제 그만해 이제 아이커 아, 이제 시끄러 그만해 이제 그만 시끄러 그만 그래 너 노래 잘해 이제 그만해 그만해 아, 봐봐 꽤가 아, 봐봐 아, 봐봐 아, 봐봐 아, 그래 그만해 노래 잘해 별이 그래 그만 그만 그만 이제 그만해 그만해 그만 별이 노래 잘해 이제 그만해 야 야 야 야 야 그만하라고 그만 시끄럽자고 이제 그만해 그래 너 노래 잘해 그만해 이제 아, 잘하네 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 아, 봐봐 내 중국에서 온 친구 마음에 들어? 이 친구가 아니라 어? 아, 봐봐 요리 요리야 어? 어? 아, 봐봐 이거 어떻게 해? 이거 이거 어떻게 해? 비리야 이거 어떻게 해? 어? 아, 잘 나왔어 괜찮아요 지금 왔어 아, 아, 아, 아, 아 잘 나왔어 잘 나왔어 잘 나왔어 잘 나왔어 아, 저한테 concurrent 컨셉을 뽑았잖아 lived houses are really crowded? wow, 우리 집이 있는 거 완전 reconnaissance 이미 쇄쇄 오, XP 그 cyber Joon cosa 저는 곧 그래서 BR where 인사 좀 해봐, 벼려 나 서로? 안녕 안녕 안녕, 해봐 이제 서로 어색하다, 근데 안녕 안녕 안녕 안녕 안녕 안녕 안녕 안녕 여기 있어봐 아빠 간다 아빠 간다 아빠 간다 왜 거기 가'),\n",
       " Document(id='a11e437d-61a0-4cf0-ba48-f9d99aa3a39e', metadata={'source': 'transcription.txt'}, page_content='아빠 간다 아빠 간다 왜 거기 가 있냐고, 이리 와봐 여기 있잖아, 여기, 친구 여기 있잖아 이리 와봐, 베리 이리 와봐, 베리 이리 와봐 이리 와봐 이리 와봐 화났어, 화났어 화났어, 화났어 오구 이놈 집에 가고 싶은 가방이야, 오로지 베리 지금 얘기 좀 해봐 너도 이제 친구가 있어야지 내 아빠만 친구 할 거야 근데 확실히 없는 움직임이 아니라 아, 뭐라고? 갑자기? 야옹이봐 우리 야옹이봐 야옹이봐 야옹이봐 야옹 야옹 지금 이제 좀 풀어졌나 보다, 기잔이 베리야, 집에 갈까? 나 썰어? 응? 아, 뭐, 핫풍하고 나리났네 집에 가자 베리야, 집에 갈까? 응 집에 갈까? 집에 가자? 집에 가자 집에 가자 안녕 안녕 안녕해 봐, 둘이 안녕해 봐 안녕해 봐 둘이야 베리야, 친구 만났지? 응? 아, 너 왜? 야, 너 왜? 야, 너 왜? 야, 너 왜? 야, 너 왜? 야, 너 왜? 내가 뭐? 나 왜? 아니, 이렇게 말해, 자르면서 나 왜? 나 왜? 나 왜? 나 왜? 아, 이쁘어 야, 오빠가 보게도 이쁘더라 아, 못봇다 응? 베리가 보게도 이쁘어? 아, 오빠가 이뻐놨어 야, 오빠가 이뻐놔 아, 오빠가 이뻐놔 아, 이 집아 또, 오빠가 이뻐놔 베리야, 팔이야, 오빠 아, 오빠 지금 다 먹었잖아 아, 오빠가 이뻐놔 아, 오빠가 이뻐놔 아아! 아 아니니 요즘은 우는게 맞들려가지고 하는 그냥 축하물어 축하면 아아 아아 아아 아아 아아 베리야 베리야 아꺼워 좀 들어봐 으음음 우지 말고 아꺼워 좀 들어봐 아 나 죽었어 아아 아아 이게 대어가 안된다 아주기랑 턱한 물어가지고 베리 아꺼워 좀 잘 들어봐 베리야 베리가 세잖아 아꺼워 대화를 못하니까 대화를 못하겠어 베리가 세잖아 베리랑 또 같이 생긴 친구 아꺼 없을 때 같이 놀면 좋지 않아? 아꺼워 싫어? 아꺼워 맨날 아빠와 나뿌네 내 못해 내 못해 내 못해 내 못해 못해 알았어 알았어 아꺼워 그래 아꺼 나 아꺼 아이고 나는 니 친구 좀 만들어 줄까 하고 그리고 갔던거지 근데 베리 다른 애들 한번 해드볼까 사람들 뭐 한번 해드볼까 아꺼워')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What is the  problem?\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The problem is that the speaker is finding the situation noisy and disruptive, with someone repeatedly saying \"아꺼워\" (akkeowo) which translates to \"I\\'m cold\" or \"It\\'s annoying\" in Korean.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is the problem?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
