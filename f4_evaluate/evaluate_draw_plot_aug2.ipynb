{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325a1267",
   "metadata": {},
   "source": [
    "# Evaluate Plots V2\n",
    "\n",
    "test pair에 대한 iteration을 수행한다.\n",
    "\n",
    "1. pickle을 통해서 모든 아이템을 다 읽는다.\n",
    "2. 현재 source sequence를 읽는다.\n",
    "3. scene similarity를 읽는다.\n",
    "- mistral로 계산한 inclusion ratio\n",
    "- 직접 계산한 entity similarity\n",
    "\n",
    "4. action sequence similarity를 계산한다.\n",
    "- agent4 필터 통과와 상관없이 플롯\n",
    "- agent4 필터를 통과하면 플롯\n",
    "\n",
    "5. activity taxnomy similarity를 계산한다.\n",
    "- agent4 필터 통과와 상관없이 플롯\n",
    "- agent4 필터를 통과하면 플롯\n",
    "\n",
    "\n",
    "다음의 pandas colum을 구성한다. (568x13 정도 되나?)\n",
    "\n",
    "없는 곳은 None표시를 한다.\n",
    "\n",
    "source idx, target idx, \n",
    "augmentation_id, inclusion_ratio, entity_similarity\n",
    "goal_category, goal_description\n",
    "core activity_gt, core activity_inf\n",
    "sequence sim, taxonomy sim\n",
    "sequence bool, taxonomy bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38f736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/project/data_annotation/goalstep/\n",
      "all: goalstep vids: 717\n",
      "all: spatial vids: 211\n",
      "dbinit: testuid excluded: goalstep vids: 646\n",
      "dbinit: testuid excluded: spatial vids: 140\n",
      "dbinit: testuid list: test goalstep vids: 71\n",
      "dbinit: testuid list: test spatial vids: 71\n",
      "dbinit: MAKE_DOCU: goalstep_document_list: 38613\n",
      "dbinit: MAKE_DOCU: goalstep_document_list: 1366\n",
      "dbinit: MAKE_DOCU: spatial_document_list: 1243\n",
      "dbinit: MAKE_DOCUAKE: spatial_document_list: 752\n",
      "LOAD FAISS GOALSTEP: /root/project/data_annotation/goalstep_docarray_faiss\n",
      "LOAD FAISS SPATIAL: /root/project/data_annotation/spatial_docarray_faiss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "%matplotlib qt\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bert_score import score\n",
    "\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath('/root/project')) # add root path to sys.path\n",
    "sys.path.append(os.path.abspath('/usr/local/lib/python3.10/dist-packages'))\n",
    "from util import util_constants\n",
    "from util import util_funcs\n",
    "import f4_evaluate.evaluate_scene as evaluate_scene\n",
    "import f1_init.database_init as database_init\n",
    "import f1_init.agent_init as agent_init\n",
    "import f1_init.constants_init as constants_init\n",
    "\n",
    "#Computing similarity\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bert_score import score\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "#PATHS\n",
    "PATH_CURR_FOLDER = os.path.abspath('') \n",
    "PATH_DATA_ANNOTATION = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_annotation'))\n",
    "PATH_DATA_INPUT_OUTPUT = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output'))\n",
    "PATH_SOURCE_TARGET_INPUT = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output/input/source_target_video_list.pkl'))\n",
    "TEST_SPATIAL_ANNOTATION_PATH_MANUAL = PATH_DATA_ANNOTATION + '/spatial_testset/manual'\n",
    "\n",
    "#PAIRSIM DATA\n",
    "PATH_PAIRSIM = PATH_DATA_ANNOTATION + '/spatial_pairsim_result'\n",
    "\n",
    "#AUGMENTED DATA PATH\n",
    "TEST_SPATIAL_ANNOTATION_PATH_SEMI = PATH_DATA_ANNOTATION + '/spatial_testset/semi'    \n",
    "TEST_SPATIAL_ANNOTATION_V2_PATH = PATH_DATA_ANNOTATION + '/spatial_augmentation/TESTSET_Augmented_Data_v2/'\n",
    "\n",
    "#BASELINE RESULT PATHS\n",
    "PATH_BASELINE_RAG = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output/output-rag-0529/'))\n",
    "PATH_BASELINE_NORAG = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output/output-norag/'))\n",
    "PATH_BASELINE_1DIRECT = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output/output-1-rag-direct/'))\n",
    "PATH_BASELINE_1GOALMEDIATION = os.path.abspath(os.path.join(PATH_CURR_FOLDER, '..', 'data_input_output/output-1-rag-goalmediation/'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d59639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    try:          \n",
    "         with open(path, \"rb\") as f:\n",
    "             data = pickle.load(f)\n",
    "             return data\n",
    "    except:\n",
    "         return None\n",
    "    \n",
    "#=====================================\n",
    "# SIMILARITIES\n",
    "#=====================================\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    if norm(vec1) == 0 or norm(vec2) == 0:\n",
    "        return 0.0\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def safe_parse_sequence(seq):\n",
    "    if seq is None:\n",
    "        return []\n",
    "    if isinstance(seq, list):\n",
    "        return seq\n",
    "    try:\n",
    "        return ast.literal_eval(seq)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def safe_parse_taxonomy(tax):\n",
    "    if tax is None:\n",
    "        return {}\n",
    "    if isinstance(tax, dict):\n",
    "        return tax\n",
    "    try:\n",
    "        return json.loads(tax)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return {}\n",
    "\n",
    "def compute_similarities(entry, embed_model):\n",
    "    # --- Parse sequences ---\n",
    "    source_seq = safe_parse_sequence(entry.get('source_sequence'))\n",
    "    target_seq = safe_parse_sequence(entry.get('target_sequence'))\n",
    "\n",
    "    if not source_seq or not target_seq:\n",
    "        seq_similarity = 0.0\n",
    "    else:\n",
    "        source_seq_str = ' '.join(source_seq)\n",
    "        target_seq_str = ' '.join(target_seq)\n",
    "        source_seq_emb = embed_model.encode(source_seq_str)\n",
    "        target_seq_emb = embed_model.encode(target_seq_str)\n",
    "        seq_similarity = cosine_similarity(source_seq_emb, target_seq_emb)\n",
    "\n",
    "    # --- Parse taxonomies ---\n",
    "    source_tax = safe_parse_taxonomy(entry.get('source_taxnomy'))\n",
    "    target_tax = safe_parse_taxonomy(entry.get('target_taxonomy'))\n",
    "\n",
    "    if not source_tax or not target_tax:\n",
    "        tax_similarity = 0.0\n",
    "    else:\n",
    "        source_tax_str = ' '.join(f\"{k}: {v}\" for k, v in source_tax.items())\n",
    "        target_tax_str = ' '.join(f\"{k}: {v}\" for k, v in target_tax.items())\n",
    "        source_tax_emb = embed_model.encode(source_tax_str)\n",
    "        target_tax_emb = embed_model.encode(target_tax_str)\n",
    "        tax_similarity = cosine_similarity(source_tax_emb, target_tax_emb)\n",
    "\n",
    "    return seq_similarity, tax_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12e164",
   "metadata": {},
   "source": [
    "## Read Results\n",
    "\n",
    "read a list of 568 pickle sets in 8 elements for 8 levels in augmentation\n",
    "\n",
    "make dictionary\n",
    "-fill every information\n",
    "-fill every calculatable information\n",
    "-save as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf074b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_list = [] # [0,0,0,0,1,1,1,1,...,283] -> [0,1,2,3,..71,0,1,2,3,4..71,...71]\n",
    "target_list = [] # [augno_0: 55, aug33_0: 33,aug67_0: 28,aug100_0: 22,...] -> [55,26,22,...]\n",
    "baselines = [\"rag\",\"norag\",\"1direct\",\"1goalmediation\"]\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def make_singlebaseline_list(baseline_result_path:str, baseline:str):\n",
    "    '''\n",
    "    func: make single baseline list\n",
    "\n",
    "    '''\n",
    "    baseline_result_path = baseline_result_path + \"/\"\n",
    "    baseline_results = []\n",
    "    augmodes = [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    length = 71 * 8 * 8 #8levels, 8files\n",
    "\n",
    "    # for i in range(length):\n",
    "    for i in range(314):\n",
    "        print(i)\n",
    "        dict = {\n",
    "            \"source_idx\":None, \"target_idx\":None, \"source_uid\": None, \"baseline\": None, \"augmode\": None, \n",
    "            \"source_sequence\": None, \"source_scene_graph\": None, \"target_scene_graph\": None, \n",
    "            \"core_activity\": None, \"source_taxonomy\": None, \"common_taxonomy\": None, \n",
    "            \"target_taxonomy\": None, \"target_sequence\": None, \n",
    "            \"sequence_boolean\": None, \"taxonomy_boolean\": None, \n",
    "            \"inclusion_ratio\":None, #\"pairwise_similarity\": None,\n",
    "            #\"sequence_similarity_sbert\": None, \"taxonomy_similarity_sbert\":None\n",
    "            }\n",
    "\n",
    "        prefix = f\"pair{i}_\"\n",
    "        augmode = i%len(augmodes)\n",
    "        # FILL RAW DATA\n",
    "        sourceinfo_dict = load_file(baseline_result_path + prefix + \"sourceinfo.pkl\")\n",
    "        targetinfo_dict = load_file(baseline_result_path + prefix + \"targetinfo.pkl\")\n",
    "        dict[\"source_idx\"] = sourceinfo_dict['source_idx']\n",
    "        dict[\"target_idx\"] = targetinfo_dict['target_idx']\n",
    "        dict[\"source_uid\"] = sourceinfo_dict['source_uid']\n",
    "        dict[\"baseline\"] = baseline\n",
    "        dict[\"augmode\"] = augmode\n",
    "\n",
    "\n",
    "        dict['source_sequence'] =sourceinfo_dict['source_action_sequence']\n",
    "        dict['source_sequence'] = dict['source_sequence'].strip('\"').split(', ') #make into list\n",
    "\n",
    "        dict['source_scene_graph'] =sourceinfo_dict['source_scene_graph']\n",
    "        dict['target_scene_graph'] =targetinfo_dict['target_scene_graph']\n",
    "\n",
    "        dict['core_activity'] =load_file(baseline_result_path + prefix + \"agent1a.pkl\")\n",
    "        dict['source_taxonomy'] =load_file(baseline_result_path + prefix + \"agent1b.pkl\")\n",
    "        dict['common_taxonomy'] =load_file(baseline_result_path + prefix + \"agent2a.pkl\")\n",
    "        dict['target_taxonomy'] =load_file(baseline_result_path + prefix + \"agent2b.pkl\")\n",
    "        dict['target_sequence'] =load_file(baseline_result_path + prefix + \"agent3.pkl\")\n",
    "\n",
    "        booleans= load_file(baseline_result_path+prefix+\"agent4.pkl\")\n",
    "        if(booleans == None):\n",
    "            dict[\"sequence_boolean\"] = False\n",
    "            dict[\"taxonomy_boolean\"] = False \n",
    "        else:\n",
    "            values = [\n",
    "                line.split(\":\")[1].strip()\n",
    "                for line in booleans.strip().splitlines()\n",
    "                if \":\" in line\n",
    "            ]\n",
    "            # print(values[1])\n",
    "            dict[\"taxonomy\"] = True if values[0] == 'yes' else False\n",
    "            dict[\"sequence_boolean\"] = True if values[1] == 'yes' else False\n",
    "            # print(dict[\"sequence_boolean\"])\n",
    "        \n",
    "        dict['inclusion_ratio'] = sourceinfo_dict['spatial_similarity']\n",
    "        #dict['pairwise_similarity'] = compare_scene_graph(dict['source_scene_graph'], dict['target_scene_graph'])\n",
    "\n",
    "        # CALCULATE SIMILARITY DATA (SBERT, BERTSCORE)\n",
    "        # SBERT for block level semantics\n",
    "\n",
    "        if dict['target_sequence'] != None:\n",
    "            dict['target_sequence'] = ast.literal_eval(dict['target_sequence'])\n",
    "            dict['target_sequence'] = [\", \".join(dict['target_sequence'])] # to single item\n",
    "            dict['source_sequence'] = [\", \".join(dict['source_sequence'])]# to single item\n",
    "            \n",
    "            sequence_similarity_sbert, taxonomy_similarity_sbert = compute_similarities(dict, sbert_model)\n",
    "            dict[\"sequence_similarity_sbert\"]= sequence_similarity_sbert\n",
    "            dict[\"taxonomy_similarity_sbert\"]= taxonomy_similarity_sbert\n",
    "\n",
    "            # BERTSCORE for little more stepwise precision in similarity measurement\n",
    "            # print(len(dict['target_sequence']))\n",
    "            # print(len(dict['source_sequence']))\n",
    "            # print(dict['target_sequence'])\n",
    "            # print(dict['source_sequence'])# not inside bracket\n",
    "            # print(dict['target_taxonomy'])\n",
    "            # print(dict['source_taxonomy'])\n",
    "\n",
    "            #takes very long\n",
    "            # dict[\"sequence_b_P\"], dict[\"sequence_b_R\"], dict[\"sequence_b_F1\"] = score(dict['target_sequence'], dict['source_sequence'], lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "            #This part gives trouble\n",
    "            # dict[\"taxonomy_b_P\"], dict[\"taxonomy_b_R\"], dict[\"taxonomy_b_F1\"] =score(dict['target_taxonomy'], dict['source_taxonomy'], lang=\"en\", rescale_with_baseline=True)\n",
    "        baseline_results.append(dict)\n",
    "    return baseline_results\n",
    "\n",
    "# baseline_rag_list = make_singlebaseline_list(PATH_BASELINE_RAG, baselines[0])\n",
    "# baseline_norag_list = make_singlebaseline_list(PATH_BASELINE_NORAG, baselines[1])\n",
    "# print(len(baseline_norag_list))\n",
    "\n",
    "# baseline_1direct_list = make_singlebaseline_list(PATH_BASELINE_1DIRECT, baselines[2])\n",
    "# print(len(baseline_1direct_list))\n",
    "\n",
    "baseline_1goalmediation_list = make_singlebaseline_list(PATH_BASELINE_1GOALMEDIATION, baselines[3])\n",
    "# print(len(baseline_1goalmediation_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcc30a",
   "metadata": {},
   "source": [
    "## Perform Numerical Calculation of Results\n",
    "\n",
    "### base results\n",
    "- pairwise scene similarity\n",
    "- seq2seq similarity\n",
    "- seq2seq taxonomy similarity\n",
    "### plot\n",
    "- per scenario similarity analysis\n",
    "- per augmentation, per scenario similarity analysis\n",
    "- per scenario similarity vs similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f81421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source_idx  target_idx                            source_uid  \\\n",
      "0         0.0        10.0  01ce4fd6-197a-4792-8778-775b03780369   \n",
      "1         0.0        10.0  01ce4fd6-197a-4792-8778-775b03780369   \n",
      "2         0.0        10.0  01ce4fd6-197a-4792-8778-775b03780369   \n",
      "3         0.0        10.0  01ce4fd6-197a-4792-8778-775b03780369   \n",
      "4         0.0        10.0  01ce4fd6-197a-4792-8778-775b03780369   \n",
      "\n",
      "         baseline  augmode                                    source_sequence  \\\n",
      "0  1goalmediation      0.0  [Arrange cloth, Arrange cloth, Arrange cloth, ...   \n",
      "1  1goalmediation      1.0  [Arrange cloth, Arrange cloth, Arrange cloth, ...   \n",
      "2  1goalmediation      2.0  [Arrange cloth, Arrange cloth, Arrange cloth, ...   \n",
      "3  1goalmediation      3.0  [Arrange cloth, Arrange cloth, Arrange cloth, ...   \n",
      "4  1goalmediation      4.0  [Arrange cloth, Arrange cloth, Arrange cloth, ...   \n",
      "\n",
      "                                  source_scene_graph  \\\n",
      "0  [{'object_id': 1, 'object_name': 'clothes hors...   \n",
      "1  [{'object_id': 1, 'object_name': 'clothes hors...   \n",
      "2  [{'object_id': 1, 'object_name': 'clothes hors...   \n",
      "3  [{'object_id': 1, 'object_name': 'clothes hors...   \n",
      "4  [{'object_id': 1, 'object_name': 'clothes hors...   \n",
      "\n",
      "                                  target_scene_graph     core_activity  \\\n",
      "0  [{'object_id': 1, 'object_name': 'avocado', 'i...    \"Tidy laundry\"   \n",
      "1  [{'object_id': 1, 'object_name': 'avocado', 'i...  Organize laundry   \n",
      "2  [{'object_id': 1, 'object_name': 'avocado', 'i...    \"Tidy laundry\"   \n",
      "3  [{'object_id': 1, 'object_name': 'avocado', 'i...      Tidy clothes   \n",
      "4  [{'object_id': 1, 'object_name': 'avocado', 'i...      Fold laundry   \n",
      "\n",
      "  source_taxonomy common_taxonomy target_taxonomy  \\\n",
      "0            None            None            None   \n",
      "1            None            None            None   \n",
      "2            None            None            None   \n",
      "3            None            None            None   \n",
      "4            None            None            None   \n",
      "\n",
      "                                     target_sequence sequence_boolean  \\\n",
      "0  [slice avocado with knife, place avocado on ch...            False   \n",
      "1  [Arrange avocado on chopping board, Arrange kn...            False   \n",
      "2  [arrange avocado on chopping board, arrange ga...            False   \n",
      "3  [Arrange avocado on chopping board, Arrange sa...            False   \n",
      "4  [slice avocado on chopping board, spread avoca...            False   \n",
      "\n",
      "  taxonomy_boolean  inclusion_ratio taxonomy  sequence_similarity_sbert  \\\n",
      "0             None              0.2    False                   0.248634   \n",
      "1             None              0.2    False                   0.467542   \n",
      "2             None              0.2    False                   0.449682   \n",
      "3             None              0.3    False                   0.622396   \n",
      "4             None              0.2    False                   0.270693   \n",
      "\n",
      "   taxonomy_similarity_sbert  \n",
      "0                        0.0  \n",
      "1                        0.0  \n",
      "2                        0.0  \n",
      "3                        0.0  \n",
      "4                        0.0  \n",
      "saved baseline list at evaluation_result_v2/1goalmediation_dict.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2478272/2970445116.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, :] = np.nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mask_nan_df(df):\n",
    "    mask = df.isna().any(axis=1)\n",
    "    df.loc[mask, :] = np.nan \n",
    "    \n",
    "def mask_nan_df_or_condition(df, columns_to_check):\n",
    "    '''\n",
    "    input: columns_to_check = [\"col1\", \"col2\"]\n",
    "    '''\n",
    "    mask = df[columns_to_check].isna().any(axis=1)\n",
    "    df.loc[mask, :] = np.nan    \n",
    "\n",
    "def save_baseline_list(path, baseline_list):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(baseline_list, f)\n",
    "        print(f\"saved baseline list at {path}\")\n",
    "\n",
    "# turn list of dictionary into dataframe\n",
    "# df_rag = pd.DataFrame(baseline_rag_list)\n",
    "\n",
    "# df_norag = pd.DataFrame(baseline_norag_list)\n",
    "# mask_nan_df(df_norag)\n",
    "\n",
    "# df_1direct = pd.DataFrame(baseline_1direct_list)\n",
    "# mask_nan_df_or_condition(df_1direct, [\"target_sequence\"])\n",
    "# print(df_1direct.head())\n",
    "\n",
    "df_1goalmediation = pd.DataFrame(baseline_1goalmediation_list)\n",
    "mask_nan_df_or_condition(df_1goalmediation, [\"target_sequence\"])\n",
    "print(df_1goalmediation.head())\n",
    "\n",
    "\n",
    "baseline_norag_path = 'evaluation_result_v2/norag_dict.pkl'\n",
    "baseline_1direct_path = 'evaluation_result_v2/1direct_dict.pkl'\n",
    "baseline_1goalmediation_list_path = 'evaluation_result_v2/1goalmediation_dict.pkl'\n",
    "\n",
    "# save_baseline_list(baseline_1direct_path, baseline_1direct_list)\n",
    "save_baseline_list(baseline_1goalmediation_list_path, baseline_1goalmediation_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a828a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "\n",
    "# Step 1: Filter where taxonomy_boolean is True\n",
    "def plot_taxonomy(df):\n",
    "    size1 = df.shape[0]\n",
    "    # df = df.dropna()\n",
    "    mybool = True\n",
    "    filtered_df = df[df['taxonomy_boolean'] == mybool]\n",
    "    size2 = filtered_df.shape[0]\n",
    "    # filtered_df = df\n",
    "\n",
    "    # Step 2: Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(filtered_df['inclusion_ratio'], filtered_df['sequence_similarity_sbert'], alpha=0.7)\n",
    "    plt.xlabel('Pairwise Similarity')\n",
    "    plt.ylabel('Taxonomy Similarity')\n",
    "    plt.title(f'Pairwise vs. Taxonomy Similarity (Only where taxonomy_boolean={mybool}) {size2}/{size1}')\n",
    "\n",
    "\n",
    "    # Generate smooth x values for curve plotting\n",
    "    N=1\n",
    "    coeffs = np.polyfit(filtered_df['inclusion_ratio'], filtered_df['taxonomy_similarity_sbert'], deg=N)\n",
    "    poly_func = np.poly1d(coeffs)\n",
    "    x_smooth = np.linspace(filtered_df['inclusion_ratio'].min(), filtered_df['inclusion_ratio'].max(), 200)\n",
    "    y_smooth = poly_func(x_smooth)\n",
    "    plt.plot(x_smooth, y_smooth, color='red', label=f'{N}° polynomial fit')\n",
    "\n",
    "\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0, 1.1)  \n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_taxonomy_cursor(df):\n",
    "    size1 = df.shape[0]\n",
    "    mybool = True\n",
    "    filtered_df = df[df['taxonomy_boolean'] == mybool]\n",
    "    size2 = filtered_df.shape[0]\n",
    "\n",
    "    # Step 2: Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sc = ax.scatter(\n",
    "        filtered_df['inclusion_ratio'], \n",
    "        filtered_df['sequence_similarity_sbert'], \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Pairwise Similarity')\n",
    "    ax.set_ylabel('Taxonomy Similarity')\n",
    "    ax.set_title(f'Pairwise vs. Taxonomy Similarity (Only where taxonomy_boolean={mybool}) {size2}/{size1}')\n",
    "\n",
    "    # Polynomial fit\n",
    "    N = 1\n",
    "    coeffs = np.polyfit(filtered_df['inclusion_ratio'], filtered_df['taxonomy_similarity_sbert'], deg=N)\n",
    "    poly_func = np.poly1d(coeffs)\n",
    "    x_smooth = np.linspace(filtered_df['inclusion_ratio'].min(), filtered_df['inclusion_ratio'].max(), 200)\n",
    "    y_smooth = poly_func(x_smooth)\n",
    "    ax.plot(x_smooth, y_smooth, color='red', label=f'{N}° polynomial fit')\n",
    "\n",
    "    # Limits and layout\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # mplcursors for interactive tooltips\n",
    "    cursor = mplcursors.cursor(sc, hover=True)\n",
    "    \n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        idx = sel.index\n",
    "        # Customize the tooltip content here\n",
    "        sel.annotation.set_text(\n",
    "            f\"x: {filtered_df['inclusion_ratio'].iloc[idx]:.2f}\\n\"\n",
    "            f\"y: {filtered_df['sequence_similarity_sbert'].iloc[idx]:.2f}\\n\"\n",
    "            f\"info: {filtered_df.get('source_uid', pd.Series(['N/A'] * size2)).iloc[idx]}\"\n",
    "        )\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "def plot_sequence(df):\n",
    "    df = df.dropna(subset=['inclusion_ratio', 'sequence_similarity_sbert', 'sequence_boolean'])\n",
    "    size1 = df.shape[0]\n",
    "    # print(df.head())\n",
    "    # df = df[df['taxonomy_boolean'] == True]\n",
    "    mybool = True\n",
    "    filtered_df = df[df['sequence_boolean'] == mybool]\n",
    "    size2 = filtered_df.shape[0]\n",
    "\n",
    "    # filtered_df = df\n",
    "\n",
    "    # Step 2: Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(filtered_df['inclusion_ratio'], filtered_df['sequence_similarity_sbert'], alpha=0.7)\n",
    "    plt.xlabel('Pairwise Similarity')\n",
    "    plt.ylabel('Sequence Similarity')\n",
    "    plt.title(f'2Pairwise vs. Sequence Similarity (Only where sequence_boolean={mybool} {size2}/{size1})')\n",
    "\n",
    "    # Generate smooth x values for curve plotting\n",
    "    N=1\n",
    "    coeffs = np.polyfit(filtered_df['inclusion_ratio'], filtered_df['sequence_similarity_sbert'], deg=N)\n",
    "    poly_func = np.poly1d(coeffs)\n",
    "    x_smooth = np.linspace(filtered_df['inclusion_ratio'].min(), filtered_df['inclusion_ratio'].max(), 200)\n",
    "    y_smooth = poly_func(x_smooth)\n",
    "    plt.plot(x_smooth, y_smooth, color='red', label=f'{N}° polynomial fit')\n",
    "\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0, 1.1)  \n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "def plot_sequence_cursor(df):\n",
    "    df = df.dropna(subset=['inclusion_ratio', 'sequence_similarity_sbert', 'sequence_boolean'])\n",
    "    size1 = df.shape[0]\n",
    "    mybool = True\n",
    "    filtered_df = df[df['sequence_boolean'] == mybool]\n",
    "    size2 = filtered_df.shape[0]\n",
    "\n",
    "    # Step 2: Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sc = ax.scatter(\n",
    "        filtered_df['inclusion_ratio'], \n",
    "        filtered_df['sequence_similarity_sbert'], \n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Pairwise Similarity')\n",
    "    ax.set_ylabel('Taxonomy Similarity')\n",
    "    ax.set_title(f'Pairwise vs. Taxonomy Similarity (Only where taxonomy_boolean={mybool}) {size2}/{size1}')\n",
    "\n",
    "    # Polynomial fit\n",
    "    # N = 1\n",
    "    # coeffs = np.polyfit(filtered_df['inclusion_ratio'], filtered_df['sequence_similarity_sbert'], deg=N)\n",
    "    # poly_func = np.poly1d(coeffs)\n",
    "    # x_smooth = np.linspace(filtered_df['inclusion_ratio'].min(), filtered_df['inclusion_ratio'].max(), 200)\n",
    "    # y_smooth = poly_func(x_smooth)\n",
    "    # ax.plot(x_smooth, y_smooth, color='red', label=f'{N}° polynomial fit')\n",
    "\n",
    "    # Limits and layout\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # mplcursors for interactive tooltips\n",
    "    cursor = mplcursors.cursor(sc, hover=True)\n",
    "    \n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        idx = sel.index\n",
    "        # Customize the tooltip content here\n",
    "        sel.annotation.set_text(\n",
    "            f\"x: {filtered_df['inclusion_ratio'].iloc[idx]:.2f}\\n\"\n",
    "            f\"y: {filtered_df['sequence_similarity_sbert'].iloc[idx]:.2f}\\n\"\n",
    "            f\"info: {filtered_df.get('source_uid', pd.Series(['N/A'] * size2)).iloc[idx]}\"\n",
    "        )\n",
    " \n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "# plot_taxonomy(df_rag)\n",
    "# plot_taxonomy(df_norag)\n",
    "# plot_taxonomy_cursor(df_norag)\n",
    "\n",
    "\n",
    "# plot_sequence(df_rag)\n",
    "# plot_sequence(df_norag)\n",
    "# plot_sequence(df_1direct)\n",
    "plot_sequence(df_1goalmediation)\n",
    "plot_sequence_cursor(df_1goalmediation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9748f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: nbagg. Using qt instead.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mplcursors\n",
    "\n",
    "# Dummy DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'inclusion_ratio': np.random.rand(10),\n",
    "    'sequence_similarity_sbert': np.random.rand(10),\n",
    "    'taxonomy_boolean': [True] * 10,\n",
    "    'source_uid': [f\"Item {i}\" for i in range(40, 50)]\n",
    "})\n",
    "\n",
    "plot_sequence_cursor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40cf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
